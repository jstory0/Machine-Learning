{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(colab)0415_MNIST_CNN구현_TF2.x.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Z3KIMWOEpDEW0zl7hG5LWPFU-7Mh-7Fx","authorship_tag":"ABX9TyPi4YnAB2dVl1XQC03vOO/x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"MXZaVQaRkB7W","executionInfo":{"status":"ok","timestamp":1649999033489,"user_tz":-540,"elapsed":7354,"user":{"displayName":"조재성","userId":"03133321578774914604"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","# 입력 Layer로 사용되는 Flatten layer, 출력, hidden layer로 사용되는 Dense layer\n","from tensorflow.keras.layers import Flatten, Dense\n","# Convolution, Pooling, Dropout-overfitting을 막기위함\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["# Raw Data Loading\n","df = pd.read_csv('/content/drive/MyDrive/Colab임시폴더/mnist/train.csv')\n","display(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"3Nv2tduxoGzu","executionInfo":{"status":"ok","timestamp":1649999044144,"user_tz":-540,"elapsed":8734,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"40f7ddc1-4661-49e0-8952-590623bf97f9"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n","0      1       0       0       0       0       0       0       0       0   \n","1      0       0       0       0       0       0       0       0       0   \n","2      1       0       0       0       0       0       0       0       0   \n","3      4       0       0       0       0       0       0       0       0   \n","4      0       0       0       0       0       0       0       0       0   \n","\n","   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0       0  ...         0         0         0         0         0         0   \n","1       0  ...         0         0         0         0         0         0   \n","2       0  ...         0         0         0         0         0         0   \n","3       0  ...         0         0         0         0         0         0   \n","4       0  ...         0         0         0         0         0         0   \n","\n","   pixel780  pixel781  pixel782  pixel783  \n","0         0         0         0         0  \n","1         0         0         0         0  \n","2         0         0         0         0  \n","3         0         0         0         0  \n","4         0         0         0         0  \n","\n","[5 rows x 785 columns]"],"text/html":["\n","  <div id=\"df-4c111039-9a67-4d64-9c9c-0911b63bcada\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel0</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>...</th>\n","      <th>pixel774</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 785 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c111039-9a67-4d64-9c9c-0911b63bcada')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4c111039-9a67-4d64-9c9c-0911b63bcada button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4c111039-9a67-4d64-9c9c-0911b63bcada');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# 데이터 전처리\n","# 결측치, 이상치, 정규화, feature engineering\n","\n","# 우리 예제는 결측치, 이상치가 없다!\n","# 대신 정규화는 필요하다!\n","\n","# train data와 test data를 분리할 필요가 있다!\n","# 학습을 하기 위한 train data\n","# 이 train data는 학습을 위한 train data와 validation data로 분리\n","\n","# 마지막 평가를 하기 위해 딱 1번 사용되는 test data\n","# 이 두개로 분리해야 한다!\n","train_x_data, test_x_data, train_t_data, test_t_data = \\\n","train_test_split(df.drop('label', axis=1, inplace=False),\n","                 df['label'],\n","                 test_size=0.3,\n","                 random_state=1,\n","                 stratify=df['label'])\n","\n","scaler = MinMaxScaler()\n","scaler.fit(train_x_data)\n","norm_train_x_data = scaler.transform(train_x_data)\n","norm_test_x_data = scaler.transform(test_x_data)\n","\n","# t_data에 대한 one-hot encoding 처리는 하지 않아도 된다!(keras 설정을 잡아서 이용)"],"metadata":{"id":"oz4EC_HcolWo","executionInfo":{"status":"ok","timestamp":1649999048709,"user_tz":-540,"elapsed":909,"user":{"displayName":"조재성","userId":"03133321578774914604"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Keras 구현\n","# model 구현\n","model = Sequential()\n","\n","model.add(Conv2D(filters=32,\n","                 kernel_size=(3,3),   # filter size\n","                 activation='relu',\n","                 input_shape=(28,28,1),   # 입력 image size, filter 개수\n","                 padding='valid',   # 원본에서 사이즈가 줄어든다.\n","                 strides=(1,1)))\n","\n","model.add(MaxPooling2D(pool_size=(2,2)))  # stride는 알아서 pool_size와 동일하게 잡는다.\n","\n","# print(model.summary())   # 내가 만든 모델의 요약정보를 볼 수 있다.\n","\n","model.add(Conv2D(filters=64,\n","                 kernel_size=(3,3),   # filter size\n","                 activation='relu',\n","                 # 입력으로 들어오는 값을 알아서 처리 -> input_shape 명시가 필요 없다.\n","                 padding='valid',   # 원본에서 사이즈가 줄어든다.\n","                 strides=(1,1)))\n","\n","model.add(MaxPooling2D(pool_size=(2,2)))  # stride는 알아서 pool_size와 동일하게 잡는다.\n","\n","model.add(Conv2D(filters=64,\n","                 kernel_size=(3,3),   # filter size\n","                 activation='relu',\n","                 # 입력으로 들어오는 값을 알아서 처리 -> input_shape 명시가 필요 없다.\n","                 padding='valid',   # 원본에서 사이즈가 줄어든다.\n","                 strides=(1,1)))\n","\n","# print(model.summary())\n","\n","model.add(Flatten())\n","model.add(Dropout(rate=0.5))   # 형태는 유지하되 연산을 수행하는 노드를 반으로 줄인다.\n","model.add(Dense(units=256,   # 256개의 node를 가지는 hidden layer\n","                activation='relu'))\n","\n","model.add(Dense(units=10,   # 10개의 Class(node)를 가지는 Output layer\n","                activation='softmax'))\n","\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRxl7XeowkQP","executionInfo":{"status":"ok","timestamp":1649999054443,"user_tz":-540,"elapsed":1524,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"dc6c8ce7-0390-4d2f-ead2-30b5c462e5f9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 32)        320       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n","                                                                 \n"," flatten (Flatten)           (None, 576)               0         \n","                                                                 \n"," dropout (Dropout)           (None, 576)               0         \n","                                                                 \n"," dense (Dense)               (None, 256)               147712    \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 206,026\n","Trainable params: 206,026\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["# model 실행 옵션\n","model.compile(optimizer=Adam(learning_rate=1e-3),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"metadata":{"id":"jzAPdCnlBbP-","executionInfo":{"status":"ok","timestamp":1649999067277,"user_tz":-540,"elapsed":250,"user":{"displayName":"조재성","userId":"03133321578774914604"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# model 학습\n","\n","# loss값과, accuracy 값이 dictionary 형태로 저장된다.\n","# norm_train_x_data 이 학습 데이터의 일부를 validation data로 활용해서\n","# 학습이 진행될 때(epoch마다) 평가를 같이 진행!\n","# 평가는 train data에 대한 loss와 accuracy,\n","#        valid data에 대한 loss와 accuracy\n","history = model.fit(norm_train_x_data.reshape(-1,28,28,1),\n","                    train_t_data,\n","                    epochs=200,       # epoch을 200번 수행\n","                    batch_size=100,   # 데이터 100개씩 순차적으로 실행\n","                    verbose=1,        # 출력 보이게\n","                    validation_split=0.3)   # input data의 30%를 valid로 사용(epoch마다)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWNRck83CtRe","executionInfo":{"status":"ok","timestamp":1649999717579,"user_tz":-540,"elapsed":40391,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"cc92fca1-89aa-4a37-f575-cbc9f454b6d4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","206/206 [==============================] - 6s 18ms/step - loss: 0.4689 - accuracy: 0.8501 - val_loss: 0.1319 - val_accuracy: 0.9585\n","Epoch 2/200\n","206/206 [==============================] - 4s 19ms/step - loss: 0.1288 - accuracy: 0.9603 - val_loss: 0.0873 - val_accuracy: 0.9744\n","Epoch 3/200\n","206/206 [==============================] - 4s 19ms/step - loss: 0.0943 - accuracy: 0.9692 - val_loss: 0.0692 - val_accuracy: 0.9766\n","Epoch 4/200\n","206/206 [==============================] - 4s 20ms/step - loss: 0.0761 - accuracy: 0.9759 - val_loss: 0.0583 - val_accuracy: 0.9811\n","Epoch 5/200\n","206/206 [==============================] - 4s 20ms/step - loss: 0.0644 - accuracy: 0.9797 - val_loss: 0.0547 - val_accuracy: 0.9832\n","Epoch 6/200\n","206/206 [==============================] - 3s 16ms/step - loss: 0.0573 - accuracy: 0.9798 - val_loss: 0.0466 - val_accuracy: 0.9871\n","Epoch 7/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0486 - accuracy: 0.9851 - val_loss: 0.0510 - val_accuracy: 0.9854\n","Epoch 8/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0442 - accuracy: 0.9857 - val_loss: 0.0453 - val_accuracy: 0.9864\n","Epoch 9/200\n","206/206 [==============================] - 3s 12ms/step - loss: 0.0384 - accuracy: 0.9873 - val_loss: 0.0486 - val_accuracy: 0.9861\n","Epoch 10/200\n","206/206 [==============================] - 3s 12ms/step - loss: 0.0371 - accuracy: 0.9879 - val_loss: 0.0397 - val_accuracy: 0.9879\n","Epoch 11/200\n","206/206 [==============================] - 3s 12ms/step - loss: 0.0324 - accuracy: 0.9888 - val_loss: 0.0417 - val_accuracy: 0.9873\n","Epoch 12/200\n","206/206 [==============================] - 3s 12ms/step - loss: 0.0281 - accuracy: 0.9900 - val_loss: 0.0462 - val_accuracy: 0.9887\n","Epoch 13/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0298 - accuracy: 0.9894 - val_loss: 0.0442 - val_accuracy: 0.9874\n","Epoch 14/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 0.0420 - val_accuracy: 0.9875\n","Epoch 15/200\n","206/206 [==============================] - 3s 12ms/step - loss: 0.0257 - accuracy: 0.9911 - val_loss: 0.0466 - val_accuracy: 0.9879\n","Epoch 16/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0224 - accuracy: 0.9926 - val_loss: 0.0440 - val_accuracy: 0.9878\n","Epoch 17/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0235 - accuracy: 0.9928 - val_loss: 0.0490 - val_accuracy: 0.9872\n","Epoch 18/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0201 - accuracy: 0.9921 - val_loss: 0.0476 - val_accuracy: 0.9885\n","Epoch 19/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 0.0450 - val_accuracy: 0.9880\n","Epoch 20/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0191 - accuracy: 0.9934 - val_loss: 0.0529 - val_accuracy: 0.9870\n","Epoch 21/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0192 - accuracy: 0.9935 - val_loss: 0.0410 - val_accuracy: 0.9878\n","Epoch 22/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0173 - accuracy: 0.9940 - val_loss: 0.0404 - val_accuracy: 0.9905\n","Epoch 23/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0153 - accuracy: 0.9951 - val_loss: 0.0424 - val_accuracy: 0.9893\n","Epoch 24/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0147 - accuracy: 0.9948 - val_loss: 0.0460 - val_accuracy: 0.9879\n","Epoch 25/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 0.0458 - val_accuracy: 0.9876\n","Epoch 26/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0135 - accuracy: 0.9956 - val_loss: 0.0404 - val_accuracy: 0.9901\n","Epoch 27/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 0.0556 - val_accuracy: 0.9888\n","Epoch 28/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0436 - val_accuracy: 0.9892\n","Epoch 29/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.0510 - val_accuracy: 0.9890\n","Epoch 30/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0137 - accuracy: 0.9950 - val_loss: 0.0479 - val_accuracy: 0.9888\n","Epoch 31/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0151 - accuracy: 0.9951 - val_loss: 0.0431 - val_accuracy: 0.9897\n","Epoch 32/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0105 - accuracy: 0.9961 - val_loss: 0.0445 - val_accuracy: 0.9887\n","Epoch 33/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0110 - accuracy: 0.9963 - val_loss: 0.0479 - val_accuracy: 0.9907\n","Epoch 34/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 0.0463 - val_accuracy: 0.9893\n","Epoch 35/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 0.0530 - val_accuracy: 0.9887\n","Epoch 36/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0469 - val_accuracy: 0.9893\n","Epoch 37/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0446 - val_accuracy: 0.9893\n","Epoch 38/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.0435 - val_accuracy: 0.9899\n","Epoch 39/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0545 - val_accuracy: 0.9896\n","Epoch 40/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0096 - accuracy: 0.9962 - val_loss: 0.0445 - val_accuracy: 0.9899\n","Epoch 41/200\n","206/206 [==============================] - 3s 16ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.0445 - val_accuracy: 0.9908\n","Epoch 42/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0080 - accuracy: 0.9970 - val_loss: 0.0530 - val_accuracy: 0.9882\n","Epoch 43/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0107 - accuracy: 0.9959 - val_loss: 0.0519 - val_accuracy: 0.9895\n","Epoch 44/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 0.0484 - val_accuracy: 0.9884\n","Epoch 45/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0512 - val_accuracy: 0.9892\n","Epoch 46/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0564 - val_accuracy: 0.9889\n","Epoch 47/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 0.0513 - val_accuracy: 0.9893\n","Epoch 48/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0097 - accuracy: 0.9966 - val_loss: 0.0510 - val_accuracy: 0.9893\n","Epoch 49/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.0484 - val_accuracy: 0.9895\n","Epoch 50/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0072 - accuracy: 0.9974 - val_loss: 0.0536 - val_accuracy: 0.9899\n","Epoch 51/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.0557 - val_accuracy: 0.9897\n","Epoch 52/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0562 - val_accuracy: 0.9901\n","Epoch 53/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0658 - val_accuracy: 0.9888\n","Epoch 54/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 0.0524 - val_accuracy: 0.9898\n","Epoch 55/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0087 - accuracy: 0.9968 - val_loss: 0.0582 - val_accuracy: 0.9873\n","Epoch 56/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0548 - val_accuracy: 0.9895\n","Epoch 57/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 0.0581 - val_accuracy: 0.9896\n","Epoch 58/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0620 - val_accuracy: 0.9895\n","Epoch 59/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0656 - val_accuracy: 0.9887\n","Epoch 60/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 0.0556 - val_accuracy: 0.9893\n","Epoch 61/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.0590 - val_accuracy: 0.9896\n","Epoch 62/200\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0483 - val_accuracy: 0.9907\n","Epoch 63/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0596 - val_accuracy: 0.9896\n","Epoch 64/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.0590 - val_accuracy: 0.9893\n","Epoch 65/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0490 - val_accuracy: 0.9908\n","Epoch 66/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.0543 - val_accuracy: 0.9900\n","Epoch 67/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0764 - val_accuracy: 0.9892\n","Epoch 68/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 0.0515 - val_accuracy: 0.9914\n","Epoch 69/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0572 - val_accuracy: 0.9898\n","Epoch 70/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0585 - val_accuracy: 0.9898\n","Epoch 71/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.0481 - val_accuracy: 0.9907\n","Epoch 72/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0039 - accuracy: 0.9984 - val_loss: 0.0581 - val_accuracy: 0.9909\n","Epoch 73/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0584 - val_accuracy: 0.9906\n","Epoch 74/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0588 - val_accuracy: 0.9909\n","Epoch 75/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0646 - val_accuracy: 0.9891\n","Epoch 76/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0588 - val_accuracy: 0.9899\n","Epoch 77/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0562 - val_accuracy: 0.9915\n","Epoch 78/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 0.0599 - val_accuracy: 0.9900\n","Epoch 79/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0571 - val_accuracy: 0.9897\n","Epoch 80/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0633 - val_accuracy: 0.9901\n","Epoch 81/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0028 - accuracy: 0.9990 - val_loss: 0.0624 - val_accuracy: 0.9902\n","Epoch 82/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0606 - val_accuracy: 0.9904\n","Epoch 83/200\n","206/206 [==============================] - 4s 21ms/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0611 - val_accuracy: 0.9897\n","Epoch 84/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0080 - accuracy: 0.9981 - val_loss: 0.0619 - val_accuracy: 0.9899\n","Epoch 85/200\n","206/206 [==============================] - 4s 21ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0659 - val_accuracy: 0.9892\n","Epoch 86/200\n","206/206 [==============================] - 5s 24ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 0.0860 - val_accuracy: 0.9874\n","Epoch 87/200\n","206/206 [==============================] - 5s 25ms/step - loss: 0.0052 - accuracy: 0.9981 - val_loss: 0.0621 - val_accuracy: 0.9895\n","Epoch 88/200\n","206/206 [==============================] - 4s 19ms/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 0.0669 - val_accuracy: 0.9891\n","Epoch 89/200\n","206/206 [==============================] - 5s 22ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0616 - val_accuracy: 0.9898\n","Epoch 90/200\n","206/206 [==============================] - 5s 22ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0723 - val_accuracy: 0.9901\n","Epoch 91/200\n","206/206 [==============================] - 4s 20ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0638 - val_accuracy: 0.9893\n","Epoch 92/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0601 - val_accuracy: 0.9906\n","Epoch 93/200\n","206/206 [==============================] - 4s 19ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0690 - val_accuracy: 0.9897\n","Epoch 94/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0042 - accuracy: 0.9984 - val_loss: 0.0623 - val_accuracy: 0.9907\n","Epoch 95/200\n","206/206 [==============================] - 4s 17ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.0497 - val_accuracy: 0.9914\n","Epoch 96/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0595 - val_accuracy: 0.9899\n","Epoch 97/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0629 - val_accuracy: 0.9913\n","Epoch 98/200\n","206/206 [==============================] - 4s 17ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0719 - val_accuracy: 0.9900\n","Epoch 99/200\n","206/206 [==============================] - 4s 17ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0584 - val_accuracy: 0.9901\n","Epoch 100/200\n","206/206 [==============================] - 3s 17ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0662 - val_accuracy: 0.9900\n","Epoch 101/200\n","206/206 [==============================] - 4s 21ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0713 - val_accuracy: 0.9885\n","Epoch 102/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0610 - val_accuracy: 0.9897\n","Epoch 103/200\n","206/206 [==============================] - 3s 17ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0608 - val_accuracy: 0.9908\n","Epoch 104/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0585 - val_accuracy: 0.9910\n","Epoch 105/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0587 - val_accuracy: 0.9910\n","Epoch 106/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0641 - val_accuracy: 0.9900\n","Epoch 107/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0643 - val_accuracy: 0.9904\n","Epoch 108/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0677 - val_accuracy: 0.9901\n","Epoch 109/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0679 - val_accuracy: 0.9905\n","Epoch 110/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.0732 - val_accuracy: 0.9899\n","Epoch 111/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0743 - val_accuracy: 0.9895\n","Epoch 112/200\n","206/206 [==============================] - 4s 21ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0602 - val_accuracy: 0.9899\n","Epoch 113/200\n","206/206 [==============================] - 4s 21ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0853 - val_accuracy: 0.9882\n","Epoch 114/200\n","206/206 [==============================] - 5s 24ms/step - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.0832 - val_accuracy: 0.9898\n","Epoch 115/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.0856 - val_accuracy: 0.9900\n","Epoch 116/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0754 - val_accuracy: 0.9899\n","Epoch 117/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0070 - accuracy: 0.9978 - val_loss: 0.0785 - val_accuracy: 0.9895\n","Epoch 118/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0624 - val_accuracy: 0.9909\n","Epoch 119/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0642 - val_accuracy: 0.9908\n","Epoch 120/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0671 - val_accuracy: 0.9892\n","Epoch 121/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0715 - val_accuracy: 0.9896\n","Epoch 122/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0793 - val_accuracy: 0.9893\n","Epoch 123/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0713 - val_accuracy: 0.9902\n","Epoch 124/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0708 - val_accuracy: 0.9901\n","Epoch 125/200\n","206/206 [==============================] - 4s 17ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0729 - val_accuracy: 0.9892\n","Epoch 126/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0703 - val_accuracy: 0.9896\n","Epoch 127/200\n","206/206 [==============================] - 4s 17ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0713 - val_accuracy: 0.9883\n","Epoch 128/200\n","206/206 [==============================] - 4s 17ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0643 - val_accuracy: 0.9902\n","Epoch 129/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0823 - val_accuracy: 0.9896\n","Epoch 130/200\n","206/206 [==============================] - 4s 21ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.0863 - val_accuracy: 0.9890\n","Epoch 131/200\n","206/206 [==============================] - 4s 18ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.0734 - val_accuracy: 0.9899\n","Epoch 132/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0699 - val_accuracy: 0.9902\n","Epoch 133/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0698 - val_accuracy: 0.9908\n","Epoch 134/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 0.0717 - val_accuracy: 0.9890\n","Epoch 135/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0653 - val_accuracy: 0.9907\n","Epoch 136/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0742 - val_accuracy: 0.9905\n","Epoch 137/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.0697 - val_accuracy: 0.9906\n","Epoch 138/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0849 - val_accuracy: 0.9897\n","Epoch 139/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.0693 - val_accuracy: 0.9904\n","Epoch 140/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0672 - val_accuracy: 0.9902\n","Epoch 141/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0735 - val_accuracy: 0.9896\n","Epoch 142/200\n","206/206 [==============================] - 3s 17ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0706 - val_accuracy: 0.9901\n","Epoch 143/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.0745 - val_accuracy: 0.9897\n","Epoch 144/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0795 - val_accuracy: 0.9893\n","Epoch 145/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0801 - val_accuracy: 0.9897\n","Epoch 146/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0807 - val_accuracy: 0.9906\n","Epoch 147/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0810 - val_accuracy: 0.9897\n","Epoch 148/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0756 - val_accuracy: 0.9896\n","Epoch 149/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0770 - val_accuracy: 0.9904\n","Epoch 150/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0881 - val_accuracy: 0.9887\n","Epoch 151/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0734 - val_accuracy: 0.9890\n","Epoch 152/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0696 - val_accuracy: 0.9902\n","Epoch 153/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0710 - val_accuracy: 0.9902\n","Epoch 154/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0716 - val_accuracy: 0.9913\n","Epoch 155/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0766 - val_accuracy: 0.9920\n","Epoch 156/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0807 - val_accuracy: 0.9909\n","Epoch 157/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.0767 - val_accuracy: 0.9900\n","Epoch 158/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0824 - val_accuracy: 0.9900\n","Epoch 159/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0721 - val_accuracy: 0.9906\n","Epoch 160/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0770 - val_accuracy: 0.9910\n","Epoch 161/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0734 - val_accuracy: 0.9899\n","Epoch 162/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0854 - val_accuracy: 0.9892\n","Epoch 163/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0918 - val_accuracy: 0.9892\n","Epoch 164/200\n","206/206 [==============================] - 4s 20ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0798 - val_accuracy: 0.9897\n","Epoch 165/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0859 - val_accuracy: 0.9902\n","Epoch 166/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0779 - val_accuracy: 0.9902\n","Epoch 167/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0736 - val_accuracy: 0.9899\n","Epoch 168/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0818 - val_accuracy: 0.9900\n","Epoch 169/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0727 - val_accuracy: 0.9902\n","Epoch 170/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0742 - val_accuracy: 0.9910\n","Epoch 171/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0845 - val_accuracy: 0.9900\n","Epoch 172/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.0759 - val_accuracy: 0.9907\n","Epoch 173/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0812 - val_accuracy: 0.9906\n","Epoch 174/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0835 - val_accuracy: 0.9905\n","Epoch 175/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0943 - val_accuracy: 0.9890\n","Epoch 176/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0771 - val_accuracy: 0.9905\n","Epoch 177/200\n","206/206 [==============================] - 3s 16ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0830 - val_accuracy: 0.9892\n","Epoch 178/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0928 - val_accuracy: 0.9889\n","Epoch 179/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.1173 - val_accuracy: 0.9897\n","Epoch 180/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0870 - val_accuracy: 0.9901\n","Epoch 181/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0766 - val_accuracy: 0.9907\n","Epoch 182/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0867 - val_accuracy: 0.9900\n","Epoch 183/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0880 - val_accuracy: 0.9895\n","Epoch 184/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.0764 - val_accuracy: 0.9899\n","Epoch 185/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.0917 - val_accuracy: 0.9892\n","Epoch 186/200\n","206/206 [==============================] - 3s 15ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0839 - val_accuracy: 0.9890\n","Epoch 187/200\n","206/206 [==============================] - 3s 14ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0752 - val_accuracy: 0.9898\n","Epoch 188/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.0883 - val_accuracy: 0.9905\n","Epoch 189/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0780 - val_accuracy: 0.9907\n","Epoch 190/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0968 - val_accuracy: 0.9891\n","Epoch 191/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0962 - val_accuracy: 0.9890\n","Epoch 192/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0875 - val_accuracy: 0.9892\n","Epoch 193/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.1203 - val_accuracy: 0.9893\n","Epoch 194/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0966 - val_accuracy: 0.9896\n","Epoch 195/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0050 - accuracy: 0.9989 - val_loss: 0.0992 - val_accuracy: 0.9891\n","Epoch 196/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.0906 - val_accuracy: 0.9896\n","Epoch 197/200\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.0924 - val_accuracy: 0.9900\n","Epoch 198/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0943 - val_accuracy: 0.9900\n","Epoch 199/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0956 - val_accuracy: 0.9896\n","Epoch 200/200\n","206/206 [==============================] - 2s 11ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.0829 - val_accuracy: 0.9895\n"]}]},{"cell_type":"code","source":["print(type(history))   # <class 'keras.callbacks.History'>\n","print(history.history.keys())  # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n","\n","plt.plot(history.history['accuracy'], color='r')\n","plt.plot(history.history['val_accuracy'], color='b')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"id":"lLcSIBJBn4mJ","executionInfo":{"status":"ok","timestamp":1649999805945,"user_tz":-540,"elapsed":383,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"ddf8c0fb-28c1-4d57-e485-8cfe44418207"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'keras.callbacks.History'>\n","dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# print(history)   # History 객체\n","# 이렇게 학습된 model 자체를 저장할 수 있다!\n","# 모델의 구조 + 계산된 모든 weight,bias를 하나의 파일에 저장 할 수 있다!\n","# 확장자는 .h5 (HDF5) 형식\n","\n","model.save('/content/drive/MyDrive/Colab임시폴더/mnist_model_save/my_mnist_model.h5')"],"metadata":{"id":"Mp36FKMeGEMa","executionInfo":{"status":"ok","timestamp":1649996036988,"user_tz":-540,"elapsed":285,"user":{"displayName":"조재성","userId":"03133321578774914604"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# model을 이용한 evaluation(평가)\n","model.evaluate(norm_test_x_data.reshape(-1,28,28,1), test_t_data)\n","#         loss               accuracy\n","# [0.11904290318489075, 0.9903967976570129]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oisizaxa-nt","executionInfo":{"status":"ok","timestamp":1649996095027,"user_tz":-540,"elapsed":2806,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"e78962f1-421c-4045-9312-9a73cbd89aa0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["394/394 [==============================] - 2s 6ms/step - loss: 0.1190 - accuracy: 0.9904\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.11904290318489075, 0.9903967976570129]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# 저장된 모델을 불러와서\n","# 성능 평가를 진행해보자!\n","from tensorflow.keras.models import load_model\n","\n","new_model = load_model('/content/drive/MyDrive/Colab임시폴더/mnist_model_save/my_mnist_model.h5')\n","\n","# model을 이용한 evaluation(평가)\n","new_model.evaluate(norm_test_x_data.reshape(-1,28,28,1), test_t_data)\n","#         loss               accuracy\n","# [0.11904290318489075, 0.9903967976570129]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-wz2vz-GcBAm","executionInfo":{"status":"ok","timestamp":1649996480920,"user_tz":-540,"elapsed":5979,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"d6be4315-36c2-4ff9-c673-54091ab83242"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["394/394 [==============================] - 5s 7ms/step - loss: 0.1190 - accuracy: 0.9904\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.11904290318489075, 0.9903967976570129]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# model 학습(ckpt, earlyStopping callback 포함)\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","# checkpoint 설정\n","# epoch마다 checkpoint 파일을 따로 만든다. 4자리의 digit 숫자로\n","checkpoint_path = '/content/drive/MyDrive/Colab임시폴더/mnist_model_save/cp-{epoch:04d}.ckpt'\n","cp_callback = ModelCheckpoint(checkpoint_path,\n","                              save_weights_only=True,\n","                              period=5,    # 5번 epoch마다 저장\n","                              verbose=1)   # 저장이 될 때마다 화면에 출력\n","\n","# earlyStopping\n","es = EarlyStopping(monitor='val_loss',\n","                   min_delta=0.001,   # 0.001은 떨어져야 정상\n","                                      # 생략하면 값이 떨어지면 유효한 것을 판단\n","                   patience=5,        # 5번 연속 값이 좋아지지 않으면 그만\n","                   verbose=1,\n","                   mode='auto',\n","                   restore_best_weights=True)\n","\n","# loss값과, accuracy 값이 dictionary 형태로 저장된다.\n","# norm_train_x_data 이 학습 데이터의 일부를 validation data로 활용해서\n","# 학습이 진행될 때(epoch마다) 평가를 같이 진행!\n","# 평가는 train data에 대한 loss와 accuracy,\n","#        valid data에 대한 loss와 accuracy\n","history = model.fit(norm_train_x_data.reshape(-1,28,28,1),\n","                    train_t_data,\n","                    epochs=50,        # epoch을 200번 수행\n","                    batch_size=100,   # 데이터 100개씩 순차적으로 실행\n","                    verbose=1,        # 출력 보이게\n","                    validation_split=0.3,   # input data의 30%를 valid로 사용(epoch마다)\n","                    callbacks=[cp_callback, es])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDHDStundro5","executionInfo":{"status":"ok","timestamp":1649997870838,"user_tz":-540,"elapsed":16543,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"0bee700c-a703-4134-a2d6-b67c4f99279a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/50\n","206/206 [==============================] - 3s 16ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0586 - val_accuracy: 0.9904\n","Epoch 2/50\n","206/206 [==============================] - 3s 12ms/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.0617 - val_accuracy: 0.9889\n","Epoch 3/50\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0052 - accuracy: 0.9979 - val_loss: 0.0592 - val_accuracy: 0.9880\n","Epoch 4/50\n","206/206 [==============================] - 2s 12ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0626 - val_accuracy: 0.9900\n","Epoch 5/50\n","203/206 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9976\n","Epoch 5: saving model to /content/drive/MyDrive/Colab임시폴더/mnist_model_save/cp-0005.ckpt\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0633 - val_accuracy: 0.9889\n","Epoch 6/50\n","205/206 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9982Restoring model weights from the end of the best epoch: 1.\n","206/206 [==============================] - 3s 13ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.0700 - val_accuracy: 0.9889\n","Epoch 6: early stopping\n"]}]},{"cell_type":"markdown","source":["#Fashion MNIST"],"metadata":{"id":"8ajudZ_o1Rg_"}},{"cell_type":"code","source":["%reset\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","# 입력 Layer로 사용되는 Flatten layer, 출력, hidden layer로 사용되는 Dense layer\n","from tensorflow.keras.layers import Flatten, Dense\n","# Convolution, Pooling, Dropout-overfitting을 막기위함\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E925HepS1G0Z","executionInfo":{"status":"ok","timestamp":1650003039499,"user_tz":-540,"elapsed":3836,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"27dadb57-d1df-4455-8bf5-2f1da3348acf"},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":["Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"]}]},{"cell_type":"code","source":["# Raw Data Loading\n","train_df = pd.read_csv('/content/drive/MyDrive/Colab임시폴더/fashion_mnist/fashion-mnist_train.csv')\n","test_df = pd.read_csv('/content/drive/MyDrive/Colab임시폴더/fashion_mnist/fashion-mnist_test.csv')\n","display(train_df.head())\n","display(test_df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":518},"id":"iE6NTlRz1OGC","executionInfo":{"status":"ok","timestamp":1650003307088,"user_tz":-540,"elapsed":4540,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"4e959baf-1d62-4966-b9f8-f3ece615f716"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n","0      2       0       0       0       0       0       0       0       0   \n","1      9       0       0       0       0       0       0       0       0   \n","2      6       0       0       0       0       0       0       0       5   \n","3      0       0       0       0       1       2       0       0       0   \n","4      3       0       0       0       0       0       0       0       0   \n","\n","   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n","0       0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n","1       0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n","2       0  ...       0.0       0.0       0.0      30.0      43.0       0.0   \n","3       0  ...       3.0       0.0       0.0       0.0       0.0       1.0   \n","4       0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n","\n","   pixel781  pixel782  pixel783  pixel784  \n","0       0.0       0.0       0.0       0.0  \n","1       0.0       0.0       0.0       0.0  \n","2       0.0       0.0       0.0       0.0  \n","3       0.0       0.0       0.0       0.0  \n","4       0.0       0.0       0.0       0.0  \n","\n","[5 rows x 785 columns]"],"text/html":["\n","  <div id=\"df-a6636a53-bc24-4c06-b899-9b7c9425b5d3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>...</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","      <th>pixel784</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>30.0</td>\n","      <td>43.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 785 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6636a53-bc24-4c06-b899-9b7c9425b5d3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a6636a53-bc24-4c06-b899-9b7c9425b5d3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a6636a53-bc24-4c06-b899-9b7c9425b5d3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n","0      0       0       0       0       0       0       0       0       9   \n","1      1       0       0       0       0       0       0       0       0   \n","2      2       0       0       0       0       0       0      14      53   \n","3      2       0       0       0       0       0       0       0       0   \n","4      3       0       0       0       0       0       0       0       0   \n","\n","   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n","0       8  ...       103        87        56         0         0         0   \n","1       0  ...        34         0         0         0         0         0   \n","2      99  ...         0         0         0         0        63        53   \n","3       0  ...       137       126       140         0       133       224   \n","4       0  ...         0         0         0         0         0         0   \n","\n","   pixel781  pixel782  pixel783  pixel784  \n","0         0         0         0         0  \n","1         0         0         0         0  \n","2        31         0         0         0  \n","3       222        56         0         0  \n","4         0         0         0         0  \n","\n","[5 rows x 785 columns]"],"text/html":["\n","  <div id=\"df-b8ad502b-1b8a-4ba8-9db5-9e1945714853\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>...</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","      <th>pixel784</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>8</td>\n","      <td>...</td>\n","      <td>103</td>\n","      <td>87</td>\n","      <td>56</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>34</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>53</td>\n","      <td>99</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>63</td>\n","      <td>53</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>137</td>\n","      <td>126</td>\n","      <td>140</td>\n","      <td>0</td>\n","      <td>133</td>\n","      <td>224</td>\n","      <td>222</td>\n","      <td>56</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 785 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8ad502b-1b8a-4ba8-9db5-9e1945714853')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b8ad502b-1b8a-4ba8-9db5-9e1945714853 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b8ad502b-1b8a-4ba8-9db5-9e1945714853');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["# x_data와 t_data로 나누기\n","train_x_data = train_df.drop('label', axis=1, inplace=False)\n","train_t_data = train_df['label']\n","test_x_data = test_df.drop('label', axis=1, inplace=False)\n","test_t_data = test_df['label']\n","\n","# 정규화\n","scaler = MinMaxScaler()\n","scaler.fit(train_x_data)\n","norm_train_x_data = scaler.transform(train_x_data)\n","norm_test_x_data = scaler.transform(test_x_data)"],"metadata":{"id":"DWk-wAxN20lo","executionInfo":{"status":"ok","timestamp":1650003548758,"user_tz":-540,"elapsed":615,"user":{"displayName":"조재성","userId":"03133321578774914604"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Keras 구현\n","# model 구현\n","model = Sequential()\n","\n","model.add(Conv2D(filters=32,\n","                 kernel_size=(3,3),   # filter size\n","                 activation='relu',\n","                 input_shape=(28,28,1),   # 입력 image size, filter 개수\n","                 padding='valid',   # 원본에서 사이즈가 줄어든다.\n","                 strides=(1,1)))\n","\n","model.add(MaxPooling2D(pool_size=(2,2)))  # stride는 알아서 pool_size와 동일하게 잡는다.\n","\n","# print(model.summary())   # 내가 만든 모델의 요약정보를 볼 수 있다.\n","\n","model.add(Conv2D(filters=64,\n","                 kernel_size=(3,3),   # filter size\n","                 activation='relu',\n","                 # 입력으로 들어오는 값을 알아서 처리 -> input_shape 명시가 필요 없다.\n","                 padding='valid',   # 원본에서 사이즈가 줄어든다.\n","                 strides=(1,1)))\n","\n","model.add(MaxPooling2D(pool_size=(2,2)))  # stride는 알아서 pool_size와 동일하게 잡는다.\n","\n","model.add(Conv2D(filters=64,\n","                 kernel_size=(3,3),   # filter size\n","                 activation='relu',\n","                 # 입력으로 들어오는 값을 알아서 처리 -> input_shape 명시가 필요 없다.\n","                 padding='valid',   # 원본에서 사이즈가 줄어든다.\n","                 strides=(1,1)))\n","\n","# print(model.summary())\n","\n","model.add(Flatten())\n","model.add(Dropout(rate=0.5))   # 형태는 유지하되 연산을 수행하는 노드를 반으로 줄인다.\n","model.add(Dense(units=256,   # 256개의 node를 가지는 hidden layer\n","                activation='relu'))\n","\n","model.add(Dense(units=10,   # 10개의 Class(node)를 가지는 Output layer\n","                activation='softmax'))\n","\n","print(model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQztzrbW4B98","executionInfo":{"status":"ok","timestamp":1650003586522,"user_tz":-540,"elapsed":233,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"1032acef-cc8a-4ae1-fa74-8acda81499ac"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 13, 13, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 11, 11, 64)        18496     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 3, 3, 64)          36928     \n","                                                                 \n"," flatten_1 (Flatten)         (None, 576)               0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 576)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 256)               147712    \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 206,026\n","Trainable params: 206,026\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["# model 실행 옵션\n","model.compile(optimizer=Adam(learning_rate=1e-3),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"metadata":{"id":"evUsF1lG4Pr1","executionInfo":{"status":"ok","timestamp":1650003611031,"user_tz":-540,"elapsed":251,"user":{"displayName":"조재성","userId":"03133321578774914604"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# model 학습\n","\n","# loss값과, accuracy 값이 dictionary 형태로 저장된다.\n","# norm_train_x_data 이 학습 데이터의 일부를 validation data로 활용해서\n","# 학습이 진행될 때(epoch마다) 평가를 같이 진행!\n","# 평가는 train data에 대한 loss와 accuracy,\n","#        valid data에 대한 loss와 accuracy\n","history = model.fit(norm_train_x_data.reshape(-1,28,28,1),\n","                    train_t_data,\n","                    epochs=200,       # epoch을 200번 수행\n","                    batch_size=100,   # 데이터 100개씩 순차적으로 실행\n","                    verbose=1,        # 출력 보이게\n","                    validation_split=0.3)   # input data의 30%를 valid로 사용(epoch마다)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nEyDtXWY4cGE","executionInfo":{"status":"ok","timestamp":1650004186266,"user_tz":-540,"elapsed":502894,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"a588e897-71e3-44fb-f2dd-e0c466e4e786"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","203/203 [==============================] - 4s 17ms/step - loss: 0.8575 - accuracy: 0.6812 - val_loss: nan - val_accuracy: 0.7703\n","Epoch 2/200\n","203/203 [==============================] - 3s 13ms/step - loss: 0.5311 - accuracy: 0.7980 - val_loss: nan - val_accuracy: 0.8260\n","Epoch 3/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.4601 - accuracy: 0.8294 - val_loss: nan - val_accuracy: 0.8448\n","Epoch 4/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.4210 - accuracy: 0.8438 - val_loss: nan - val_accuracy: 0.8636\n","Epoch 5/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.3896 - accuracy: 0.8574 - val_loss: nan - val_accuracy: 0.8706\n","Epoch 6/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.3663 - accuracy: 0.8644 - val_loss: nan - val_accuracy: 0.8776\n","Epoch 7/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.3483 - accuracy: 0.8715 - val_loss: nan - val_accuracy: 0.8812\n","Epoch 8/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.3233 - accuracy: 0.8792 - val_loss: nan - val_accuracy: 0.8796\n","Epoch 9/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.3234 - accuracy: 0.8799 - val_loss: nan - val_accuracy: 0.8781\n","Epoch 10/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.3025 - accuracy: 0.8857 - val_loss: nan - val_accuracy: 0.8911\n","Epoch 11/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.2882 - accuracy: 0.8934 - val_loss: nan - val_accuracy: 0.8920\n","Epoch 12/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.2789 - accuracy: 0.8965 - val_loss: nan - val_accuracy: 0.8912\n","Epoch 13/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.2649 - accuracy: 0.8999 - val_loss: nan - val_accuracy: 0.9008\n","Epoch 14/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.2561 - accuracy: 0.9031 - val_loss: nan - val_accuracy: 0.9016\n","Epoch 15/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.2441 - accuracy: 0.9087 - val_loss: nan - val_accuracy: 0.8992\n","Epoch 16/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.2392 - accuracy: 0.9098 - val_loss: nan - val_accuracy: 0.8972\n","Epoch 17/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.2467 - accuracy: 0.9041 - val_loss: nan - val_accuracy: 0.9001\n","Epoch 18/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.2214 - accuracy: 0.9171 - val_loss: nan - val_accuracy: 0.9035\n","Epoch 19/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.2182 - accuracy: 0.9172 - val_loss: nan - val_accuracy: 0.9028\n","Epoch 20/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.2038 - accuracy: 0.9243 - val_loss: nan - val_accuracy: 0.9089\n","Epoch 21/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.2053 - accuracy: 0.9226 - val_loss: nan - val_accuracy: 0.9001\n","Epoch 22/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1940 - accuracy: 0.9276 - val_loss: nan - val_accuracy: 0.9097\n","Epoch 23/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1855 - accuracy: 0.9274 - val_loss: nan - val_accuracy: 0.9048\n","Epoch 24/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1905 - accuracy: 0.9276 - val_loss: nan - val_accuracy: 0.9037\n","Epoch 25/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1712 - accuracy: 0.9341 - val_loss: nan - val_accuracy: 0.9054\n","Epoch 26/200\n","203/203 [==============================] - 3s 13ms/step - loss: 0.1726 - accuracy: 0.9359 - val_loss: nan - val_accuracy: 0.9100\n","Epoch 27/200\n","203/203 [==============================] - 3s 14ms/step - loss: 0.1662 - accuracy: 0.9353 - val_loss: nan - val_accuracy: 0.9064\n","Epoch 28/200\n","203/203 [==============================] - 3s 14ms/step - loss: 0.1629 - accuracy: 0.9394 - val_loss: nan - val_accuracy: 0.9067\n","Epoch 29/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1578 - accuracy: 0.9409 - val_loss: nan - val_accuracy: 0.9079\n","Epoch 30/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1529 - accuracy: 0.9409 - val_loss: nan - val_accuracy: 0.9062\n","Epoch 31/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1592 - accuracy: 0.9373 - val_loss: nan - val_accuracy: 0.9006\n","Epoch 32/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1432 - accuracy: 0.9446 - val_loss: nan - val_accuracy: 0.9057\n","Epoch 33/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1347 - accuracy: 0.9487 - val_loss: nan - val_accuracy: 0.9054\n","Epoch 34/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1279 - accuracy: 0.9508 - val_loss: nan - val_accuracy: 0.9118\n","Epoch 35/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1269 - accuracy: 0.9512 - val_loss: nan - val_accuracy: 0.9079\n","Epoch 36/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1361 - accuracy: 0.9471 - val_loss: nan - val_accuracy: 0.9021\n","Epoch 37/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1231 - accuracy: 0.9541 - val_loss: nan - val_accuracy: 0.9010\n","Epoch 38/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1171 - accuracy: 0.9541 - val_loss: nan - val_accuracy: 0.9068\n","Epoch 39/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.1261 - accuracy: 0.9516 - val_loss: nan - val_accuracy: 0.9081\n","Epoch 40/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1164 - accuracy: 0.9565 - val_loss: nan - val_accuracy: 0.9119\n","Epoch 41/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1061 - accuracy: 0.9589 - val_loss: nan - val_accuracy: 0.9048\n","Epoch 42/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1115 - accuracy: 0.9585 - val_loss: nan - val_accuracy: 0.9083\n","Epoch 43/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.1070 - accuracy: 0.9592 - val_loss: nan - val_accuracy: 0.9094\n","Epoch 44/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1033 - accuracy: 0.9598 - val_loss: nan - val_accuracy: 0.9079\n","Epoch 45/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.1026 - accuracy: 0.9603 - val_loss: nan - val_accuracy: 0.9069\n","Epoch 46/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0933 - accuracy: 0.9640 - val_loss: nan - val_accuracy: 0.9083\n","Epoch 47/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0897 - accuracy: 0.9662 - val_loss: nan - val_accuracy: 0.9089\n","Epoch 48/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0971 - accuracy: 0.9628 - val_loss: nan - val_accuracy: 0.9090\n","Epoch 49/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0891 - accuracy: 0.9664 - val_loss: nan - val_accuracy: 0.9087\n","Epoch 50/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0922 - accuracy: 0.9645 - val_loss: nan - val_accuracy: 0.9128\n","Epoch 51/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0867 - accuracy: 0.9659 - val_loss: nan - val_accuracy: 0.9106\n","Epoch 52/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0841 - accuracy: 0.9665 - val_loss: nan - val_accuracy: 0.9090\n","Epoch 53/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0870 - accuracy: 0.9682 - val_loss: nan - val_accuracy: 0.9048\n","Epoch 54/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0806 - accuracy: 0.9686 - val_loss: nan - val_accuracy: 0.9058\n","Epoch 55/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0763 - accuracy: 0.9709 - val_loss: nan - val_accuracy: 0.9083\n","Epoch 56/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0900 - accuracy: 0.9662 - val_loss: nan - val_accuracy: 0.9027\n","Epoch 57/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0844 - accuracy: 0.9679 - val_loss: nan - val_accuracy: 0.9048\n","Epoch 58/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0758 - accuracy: 0.9720 - val_loss: nan - val_accuracy: 0.9122\n","Epoch 59/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0725 - accuracy: 0.9719 - val_loss: nan - val_accuracy: 0.9096\n","Epoch 60/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0765 - accuracy: 0.9712 - val_loss: nan - val_accuracy: 0.9055\n","Epoch 61/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0701 - accuracy: 0.9730 - val_loss: nan - val_accuracy: 0.9050\n","Epoch 62/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0699 - accuracy: 0.9739 - val_loss: nan - val_accuracy: 0.9088\n","Epoch 63/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0683 - accuracy: 0.9740 - val_loss: nan - val_accuracy: 0.9046\n","Epoch 64/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0722 - accuracy: 0.9728 - val_loss: nan - val_accuracy: 0.9074\n","Epoch 65/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0662 - accuracy: 0.9749 - val_loss: nan - val_accuracy: 0.9091\n","Epoch 66/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0668 - accuracy: 0.9756 - val_loss: nan - val_accuracy: 0.9110\n","Epoch 67/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0614 - accuracy: 0.9776 - val_loss: nan - val_accuracy: 0.9076\n","Epoch 68/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0595 - accuracy: 0.9781 - val_loss: nan - val_accuracy: 0.9098\n","Epoch 69/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0601 - accuracy: 0.9772 - val_loss: nan - val_accuracy: 0.9134\n","Epoch 70/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0645 - accuracy: 0.9750 - val_loss: nan - val_accuracy: 0.9091\n","Epoch 71/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0819 - accuracy: 0.9689 - val_loss: nan - val_accuracy: 0.9073\n","Epoch 72/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0597 - accuracy: 0.9772 - val_loss: nan - val_accuracy: 0.9094\n","Epoch 73/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0593 - accuracy: 0.9774 - val_loss: nan - val_accuracy: 0.9075\n","Epoch 74/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0624 - accuracy: 0.9772 - val_loss: nan - val_accuracy: 0.9090\n","Epoch 75/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0569 - accuracy: 0.9783 - val_loss: nan - val_accuracy: 0.9047\n","Epoch 76/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0553 - accuracy: 0.9796 - val_loss: nan - val_accuracy: 0.9064\n","Epoch 77/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0606 - accuracy: 0.9769 - val_loss: nan - val_accuracy: 0.9073\n","Epoch 78/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0552 - accuracy: 0.9793 - val_loss: nan - val_accuracy: 0.9073\n","Epoch 79/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0581 - accuracy: 0.9782 - val_loss: nan - val_accuracy: 0.9091\n","Epoch 80/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0557 - accuracy: 0.9790 - val_loss: nan - val_accuracy: 0.9074\n","Epoch 81/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0605 - accuracy: 0.9790 - val_loss: nan - val_accuracy: 0.9053\n","Epoch 82/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0565 - accuracy: 0.9781 - val_loss: nan - val_accuracy: 0.9054\n","Epoch 83/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0674 - accuracy: 0.9738 - val_loss: nan - val_accuracy: 0.9073\n","Epoch 84/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0649 - accuracy: 0.9754 - val_loss: nan - val_accuracy: 0.9092\n","Epoch 85/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0520 - accuracy: 0.9808 - val_loss: nan - val_accuracy: 0.9090\n","Epoch 86/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0495 - accuracy: 0.9813 - val_loss: nan - val_accuracy: 0.9087\n","Epoch 87/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0493 - accuracy: 0.9819 - val_loss: nan - val_accuracy: 0.9100\n","Epoch 88/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0471 - accuracy: 0.9835 - val_loss: nan - val_accuracy: 0.9053\n","Epoch 89/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0542 - accuracy: 0.9794 - val_loss: nan - val_accuracy: 0.9077\n","Epoch 90/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0479 - accuracy: 0.9819 - val_loss: nan - val_accuracy: 0.9039\n","Epoch 91/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0549 - accuracy: 0.9802 - val_loss: nan - val_accuracy: 0.9072\n","Epoch 92/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0483 - accuracy: 0.9830 - val_loss: nan - val_accuracy: 0.9106\n","Epoch 93/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0468 - accuracy: 0.9830 - val_loss: nan - val_accuracy: 0.9107\n","Epoch 94/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0469 - accuracy: 0.9828 - val_loss: nan - val_accuracy: 0.9104\n","Epoch 95/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0561 - accuracy: 0.9795 - val_loss: nan - val_accuracy: 0.9104\n","Epoch 96/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0470 - accuracy: 0.9827 - val_loss: nan - val_accuracy: 0.9080\n","Epoch 97/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0503 - accuracy: 0.9814 - val_loss: nan - val_accuracy: 0.9091\n","Epoch 98/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0453 - accuracy: 0.9826 - val_loss: nan - val_accuracy: 0.9083\n","Epoch 99/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0446 - accuracy: 0.9835 - val_loss: nan - val_accuracy: 0.9097\n","Epoch 100/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0464 - accuracy: 0.9831 - val_loss: nan - val_accuracy: 0.9065\n","Epoch 101/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0581 - accuracy: 0.9782 - val_loss: nan - val_accuracy: 0.9102\n","Epoch 102/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0441 - accuracy: 0.9839 - val_loss: nan - val_accuracy: 0.9046\n","Epoch 103/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0496 - accuracy: 0.9816 - val_loss: nan - val_accuracy: 0.9109\n","Epoch 104/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0383 - accuracy: 0.9866 - val_loss: nan - val_accuracy: 0.9107\n","Epoch 105/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0400 - accuracy: 0.9846 - val_loss: nan - val_accuracy: 0.9079\n","Epoch 106/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0489 - accuracy: 0.9822 - val_loss: nan - val_accuracy: 0.9058\n","Epoch 107/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0486 - accuracy: 0.9825 - val_loss: nan - val_accuracy: 0.9069\n","Epoch 108/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0410 - accuracy: 0.9853 - val_loss: nan - val_accuracy: 0.9088\n","Epoch 109/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0441 - accuracy: 0.9835 - val_loss: nan - val_accuracy: 0.9051\n","Epoch 110/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0436 - accuracy: 0.9840 - val_loss: nan - val_accuracy: 0.9069\n","Epoch 111/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0488 - accuracy: 0.9818 - val_loss: nan - val_accuracy: 0.9064\n","Epoch 112/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0437 - accuracy: 0.9839 - val_loss: nan - val_accuracy: 0.9067\n","Epoch 113/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0439 - accuracy: 0.9830 - val_loss: nan - val_accuracy: 0.9072\n","Epoch 114/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0445 - accuracy: 0.9847 - val_loss: nan - val_accuracy: 0.9052\n","Epoch 115/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0401 - accuracy: 0.9854 - val_loss: nan - val_accuracy: 0.9066\n","Epoch 116/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0415 - accuracy: 0.9852 - val_loss: nan - val_accuracy: 0.9052\n","Epoch 117/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0386 - accuracy: 0.9858 - val_loss: nan - val_accuracy: 0.9062\n","Epoch 118/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0445 - accuracy: 0.9834 - val_loss: nan - val_accuracy: 0.9100\n","Epoch 119/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0393 - accuracy: 0.9871 - val_loss: nan - val_accuracy: 0.9079\n","Epoch 120/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0423 - accuracy: 0.9846 - val_loss: nan - val_accuracy: 0.9054\n","Epoch 121/200\n","203/203 [==============================] - 3s 12ms/step - loss: 0.0396 - accuracy: 0.9853 - val_loss: nan - val_accuracy: 0.9082\n","Epoch 122/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0386 - accuracy: 0.9857 - val_loss: nan - val_accuracy: 0.9102\n","Epoch 123/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0388 - accuracy: 0.9859 - val_loss: nan - val_accuracy: 0.9090\n","Epoch 124/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0389 - accuracy: 0.9862 - val_loss: nan - val_accuracy: 0.9098\n","Epoch 125/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0362 - accuracy: 0.9864 - val_loss: nan - val_accuracy: 0.9072\n","Epoch 126/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0383 - accuracy: 0.9867 - val_loss: nan - val_accuracy: 0.9083\n","Epoch 127/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0423 - accuracy: 0.9843 - val_loss: nan - val_accuracy: 0.9102\n","Epoch 128/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0342 - accuracy: 0.9884 - val_loss: nan - val_accuracy: 0.9069\n","Epoch 129/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0376 - accuracy: 0.9875 - val_loss: nan - val_accuracy: 0.9110\n","Epoch 130/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: nan - val_accuracy: 0.9092\n","Epoch 131/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0454 - accuracy: 0.9842 - val_loss: nan - val_accuracy: 0.9096\n","Epoch 132/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0332 - accuracy: 0.9878 - val_loss: nan - val_accuracy: 0.9110\n","Epoch 133/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0499 - accuracy: 0.9825 - val_loss: nan - val_accuracy: 0.9089\n","Epoch 134/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: nan - val_accuracy: 0.9066\n","Epoch 135/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0401 - accuracy: 0.9846 - val_loss: nan - val_accuracy: 0.9104\n","Epoch 136/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0341 - accuracy: 0.9880 - val_loss: nan - val_accuracy: 0.9076\n","Epoch 137/200\n","203/203 [==============================] - 3s 12ms/step - loss: 0.0337 - accuracy: 0.9875 - val_loss: nan - val_accuracy: 0.9068\n","Epoch 138/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0436 - accuracy: 0.9848 - val_loss: nan - val_accuracy: 0.9044\n","Epoch 139/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0346 - accuracy: 0.9879 - val_loss: nan - val_accuracy: 0.9055\n","Epoch 140/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0495 - accuracy: 0.9825 - val_loss: nan - val_accuracy: 0.9073\n","Epoch 141/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0387 - accuracy: 0.9859 - val_loss: nan - val_accuracy: 0.9053\n","Epoch 142/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0385 - accuracy: 0.9859 - val_loss: nan - val_accuracy: 0.9092\n","Epoch 143/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0381 - accuracy: 0.9854 - val_loss: nan - val_accuracy: 0.9087\n","Epoch 144/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0321 - accuracy: 0.9884 - val_loss: nan - val_accuracy: 0.9074\n","Epoch 145/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0317 - accuracy: 0.9882 - val_loss: nan - val_accuracy: 0.9070\n","Epoch 146/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0382 - accuracy: 0.9863 - val_loss: nan - val_accuracy: 0.9085\n","Epoch 147/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: nan - val_accuracy: 0.9062\n","Epoch 148/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0351 - accuracy: 0.9875 - val_loss: nan - val_accuracy: 0.9062\n","Epoch 149/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0369 - accuracy: 0.9871 - val_loss: nan - val_accuracy: 0.9048\n","Epoch 150/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0343 - accuracy: 0.9880 - val_loss: nan - val_accuracy: 0.9069\n","Epoch 151/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0362 - accuracy: 0.9877 - val_loss: nan - val_accuracy: 0.9020\n","Epoch 152/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0328 - accuracy: 0.9887 - val_loss: nan - val_accuracy: 0.9052\n","Epoch 153/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0366 - accuracy: 0.9866 - val_loss: nan - val_accuracy: 0.9051\n","Epoch 154/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0374 - accuracy: 0.9868 - val_loss: nan - val_accuracy: 0.9010\n","Epoch 155/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0316 - accuracy: 0.9902 - val_loss: nan - val_accuracy: 0.9072\n","Epoch 156/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0356 - accuracy: 0.9877 - val_loss: nan - val_accuracy: 0.9080\n","Epoch 157/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0322 - accuracy: 0.9885 - val_loss: nan - val_accuracy: 0.9060\n","Epoch 158/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0394 - accuracy: 0.9869 - val_loss: nan - val_accuracy: 0.9037\n","Epoch 159/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0393 - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9059\n","Epoch 160/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0379 - accuracy: 0.9875 - val_loss: nan - val_accuracy: 0.9052\n","Epoch 161/200\n","203/203 [==============================] - 2s 10ms/step - loss: 0.0384 - accuracy: 0.9870 - val_loss: nan - val_accuracy: 0.9068\n","Epoch 162/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0303 - accuracy: 0.9893 - val_loss: nan - val_accuracy: 0.9053\n","Epoch 163/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0308 - accuracy: 0.9891 - val_loss: nan - val_accuracy: 0.9074\n","Epoch 164/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0343 - accuracy: 0.9881 - val_loss: nan - val_accuracy: 0.9079\n","Epoch 165/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0362 - accuracy: 0.9873 - val_loss: nan - val_accuracy: 0.9073\n","Epoch 166/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0313 - accuracy: 0.9885 - val_loss: nan - val_accuracy: 0.9076\n","Epoch 167/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0308 - accuracy: 0.9900 - val_loss: nan - val_accuracy: 0.9058\n","Epoch 168/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0616 - accuracy: 0.9793 - val_loss: nan - val_accuracy: 0.9046\n","Epoch 169/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0316 - accuracy: 0.9886 - val_loss: nan - val_accuracy: 0.9051\n","Epoch 170/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0271 - accuracy: 0.9903 - val_loss: nan - val_accuracy: 0.9076\n","Epoch 171/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0306 - accuracy: 0.9884 - val_loss: nan - val_accuracy: 0.9066\n","Epoch 172/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0319 - accuracy: 0.9891 - val_loss: nan - val_accuracy: 0.9092\n","Epoch 173/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0332 - accuracy: 0.9887 - val_loss: nan - val_accuracy: 0.9082\n","Epoch 174/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: nan - val_accuracy: 0.9047\n","Epoch 175/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0305 - accuracy: 0.9894 - val_loss: nan - val_accuracy: 0.9072\n","Epoch 176/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0273 - accuracy: 0.9904 - val_loss: nan - val_accuracy: 0.9070\n","Epoch 177/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0298 - accuracy: 0.9899 - val_loss: nan - val_accuracy: 0.9065\n","Epoch 178/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0293 - accuracy: 0.9900 - val_loss: nan - val_accuracy: 0.9038\n","Epoch 179/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0315 - accuracy: 0.9890 - val_loss: nan - val_accuracy: 0.9083\n","Epoch 180/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: nan - val_accuracy: 0.9097\n","Epoch 181/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0320 - accuracy: 0.9886 - val_loss: nan - val_accuracy: 0.9083\n","Epoch 182/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0286 - accuracy: 0.9905 - val_loss: nan - val_accuracy: 0.9097\n","Epoch 183/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0301 - accuracy: 0.9900 - val_loss: nan - val_accuracy: 0.9082\n","Epoch 184/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0334 - accuracy: 0.9884 - val_loss: nan - val_accuracy: 0.9017\n","Epoch 185/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0313 - accuracy: 0.9893 - val_loss: nan - val_accuracy: 0.9102\n","Epoch 186/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0314 - accuracy: 0.9896 - val_loss: nan - val_accuracy: 0.9092\n","Epoch 187/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0329 - accuracy: 0.9886 - val_loss: nan - val_accuracy: 0.9088\n","Epoch 188/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: nan - val_accuracy: 0.9095\n","Epoch 189/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0301 - accuracy: 0.9891 - val_loss: nan - val_accuracy: 0.9074\n","Epoch 190/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0288 - accuracy: 0.9898 - val_loss: nan - val_accuracy: 0.9061\n","Epoch 191/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0375 - accuracy: 0.9873 - val_loss: nan - val_accuracy: 0.9044\n","Epoch 192/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0341 - accuracy: 0.9884 - val_loss: nan - val_accuracy: 0.9098\n","Epoch 193/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0254 - accuracy: 0.9916 - val_loss: nan - val_accuracy: 0.9083\n","Epoch 194/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0282 - accuracy: 0.9904 - val_loss: nan - val_accuracy: 0.9104\n","Epoch 195/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0483 - accuracy: 0.9830 - val_loss: nan - val_accuracy: 0.9074\n","Epoch 196/200\n","203/203 [==============================] - 3s 12ms/step - loss: 0.0308 - accuracy: 0.9891 - val_loss: nan - val_accuracy: 0.9030\n","Epoch 197/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: nan - val_accuracy: 0.9074\n","Epoch 198/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: nan - val_accuracy: 0.9077\n","Epoch 199/200\n","203/203 [==============================] - 2s 12ms/step - loss: 0.0258 - accuracy: 0.9902 - val_loss: nan - val_accuracy: 0.9088\n","Epoch 200/200\n","203/203 [==============================] - 2s 11ms/step - loss: 0.0298 - accuracy: 0.9895 - val_loss: nan - val_accuracy: 0.9094\n"]}]},{"cell_type":"code","source":["# model을 이용한 evaluation(평가)\n","model.evaluate(norm_test_x_data.reshape(-1,28,28,1), test_t_data)\n","#         loss               accuracy\n","# [0.11904290318489075, 0.9903967976570129]\n","# [0.5086526274681091, 0.9111999869346619]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qn_XwL404xG7","executionInfo":{"status":"ok","timestamp":1650004520519,"user_tz":-540,"elapsed":2875,"user":{"displayName":"조재성","userId":"03133321578774914604"}},"outputId":"952a6f5a-f6d4-43f3-8135-7d58a8ffef69"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.5087 - accuracy: 0.9112\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5086526274681091, 0.9111999869346619]"]},"metadata":{},"execution_count":19}]}]}
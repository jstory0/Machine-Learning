{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ea5bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MNIST 예제를 구현해보자!\n",
    "# Data는 Kaggle에서 다운로드 한다!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('../data/mnist/train.csv')\n",
    "display(df.shape)   # (42000, 785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ac4164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb57d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# 결측치나 이상치가 존재하지 않는다.\n",
    "# 단, 정규화는 필요하다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297468d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이미지 확인\n",
    "figure = plt.figure()   # 도화지 준비\n",
    "ax_arr = []   # python list\n",
    "\n",
    "img_data = df.drop('label', axis=1, inplace=False).values   # 2차원 ndarray로 빼낸다.\n",
    "\n",
    "for n in range(10):\n",
    "    ax_arr.append(figure.add_subplot(2,5,n+1))\n",
    "    ax_arr[n].imshow(img_data[n].reshape(28,28),\n",
    "                     cmap='Greys',              # 흑백이미지 표현\n",
    "                     interpolation='nearest')   # 보간법\n",
    "\n",
    "plt.tight_layout()   # subplot간의 간격을 준다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0f8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab468769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss val : 1.2059354782104492\n",
      "loss val : 0.24192436039447784\n",
      "loss val : 0.22665485739707947\n",
      "loss val : 0.21622036397457123\n",
      "loss val : 0.20953845977783203\n",
      "loss val : 0.2049787938594818\n",
      "loss val : 0.20144236087799072\n",
      "loss val : 0.19850170612335205\n",
      "loss val : 0.19604459404945374\n",
      "loss val : 0.1940048336982727\n"
     ]
    }
   ],
   "source": [
    "## Tensorflow Implementation ##\n",
    "sess = tf.Session()\n",
    "\n",
    "onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=10))\n",
    "onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=10))\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]))\n",
    "b = tf.Variable(tf.random.normal([10]))\n",
    "\n",
    "# Hypothesis, Model\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 반복학습\n",
    "num_of_epoch = 1000\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    \n",
    "    total_batch = int(norm_train_x_data.shape[0] / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_x = norm_train_x_data[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = onehot_train_t_data[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        _, loss_val = sess.run([train, loss], feed_dict={X:batch_x,\n",
    "                                                         T:batch_y})\n",
    "        \n",
    "    if step % 100 == 0:\n",
    "        print('loss val : {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e85ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.908650815486908\n"
     ]
    }
   ],
   "source": [
    "# accuracy 측정\n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "accuracy_val = sess.run(accuracy, feed_dict={X:norm_test_x_data,\n",
    "                                             T:onehot_test_t_data})\n",
    "print('Accuracy : {}'.format(accuracy_val))   # 0.908650815486908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506ecaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[1.6759058]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)   # 2.3.0\n",
    "\n",
    "W = tf.random.normal([1], dtype=tf.float32)\n",
    "\n",
    "# 1.15 버전에서 W의 값을 알아내려면 session을 통해서 node를 실행시켜서\n",
    "# 값을 얻어야 한다.\n",
    "# 2.x 버전은 eager excution(즉시실행모드)을 지원한다.\n",
    "# session이 필요가 없고 일반적인 프로그래밍 하는 것처럼 사용할 수 있다.\n",
    "print(W.numpy())\n",
    "\n",
    "# 추가적으로 초기화 하는 코드 역시 불필요해서 이제는 사용하지 않는다.\n",
    "# sess.run(tf.global_variables_initializer())   # 사용하지 않는다.\n",
    "\n",
    "# placeholder도 역시 삭제되었다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a614c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그러면 Keras를 사용한다고 하는데..\n",
    "# 코드는 어떻게 작성하는 건가요?\n",
    "\n",
    "# 그림과 매칭해서 봐야한다!\n",
    "\n",
    "# keras의 model은 어떻게 만드나요?\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential()   # keras의 Sequential model을 만든다.\n",
    "\n",
    "# model.add()를 이용해서 layer를 추가해주자! 밑은 의미상의 코드\n",
    "# model.add('input layer')\n",
    "# model.add('output layer')\n",
    "\n",
    "# loss의 종류와 optimizer 종류를 설정\n",
    "# model.compile()\n",
    "\n",
    "# 학습 (마치 sklearn 사용하는 것처럼..)\n",
    "# model.fit()\n",
    "\n",
    "# 평가와 predict\n",
    "# model.evaluate()  => 모델 평가\n",
    "# model.predict()   => 예측값 도출\n",
    "\n",
    "# 모델 저장\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d9d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reset\n",
    "# 대표적인 multinomial 예제인 MNIST를 이용해서\n",
    "# Tensorflow 2.x 버전으로 구현해보자!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential   # keras model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(input Layer)\n",
    "                                                     # Dense(Output Layer)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Raw Data Loading\n",
    "\n",
    "df = pd. read_csv('../data/mnist/train.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8a0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "964886b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 2.x 구현\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()   # model 박스 하나 만듬\n",
    "\n",
    "# print(norm_train_x_data.shape)   # (29400, 784)\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "# input layer는 사실 하는일이 없다! 그래서 코드를 나누지 않고 한번에\n",
    "# 기술할 수도 있다! (이건 나중에 진행)\n",
    "\n",
    "print(model.summary())\n",
    "# dense_2 (Dense)              (None, 10)                7850 => bias까지 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b51a2d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 5s 21ms/step - loss: 2.2438 - accuracy: 0.1567 - val_loss: 2.1256 - val_accuracy: 0.2393\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 3s 13ms/step - loss: 2.0156 - accuracy: 0.3484 - val_loss: 1.9243 - val_accuracy: 0.4344\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 4s 16ms/step - loss: 1.8323 - accuracy: 0.5137 - val_loss: 1.7566 - val_accuracy: 0.5707: 1.8334 - accuracy: 0.51\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 3s 11ms/step - loss: 1.6792 - accuracy: 0.6193 - val_loss: 1.6160 - val_accuracy: 0.6534\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 3s 14ms/step - loss: 1.5506 - accuracy: 0.6819 - val_loss: 1.4976 - val_accuracy: 0.7029- ETA: 0s - loss: 1.5678 - accura - ETA: 0s - loss: 1.5634 -  - ETA: 0s - loss: 1.5532 - accuracy: \n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 3s 11ms/step - loss: 1.4423 - accuracy: 0.7194 - val_loss: 1.3977 - val_accuracy: 0.7332\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 1.3506 - accuracy: 0.7436 - val_loss: 1.3127 - val_accuracy: 0.7548\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 1.2724 - accuracy: 0.7622 - val_loss: 1.2400 - val_accuracy: 0.7682\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 1.2053 - accuracy: 0.7739 - val_loss: 1.1772 - val_accuracy: 0.7821\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 1.1472 - accuracy: 0.7839 - val_loss: 1.1229 - val_accuracy: 0.7903\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 1.0966 - accuracy: 0.7927 - val_loss: 1.0753 - val_accuracy: 0.7956\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 1.0522 - accuracy: 0.7993 - val_loss: 1.0335 - val_accuracy: 0.80390s - loss: 1.0554 - accuracy: \n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 1.0129 - accuracy: 0.8048 - val_loss: 0.9964 - val_accuracy: 0.8094\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.9780 - accuracy: 0.8094 - val_loss: 0.9632 - val_accuracy: 0.8126\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.9467 - accuracy: 0.8142 - val_loss: 0.9334 - val_accuracy: 0.8168499 - ac\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.9185 - accuracy: 0.8175 - val_loss: 0.9066 - val_accuracy: 0.8214\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 3s 11ms/step - loss: 0.8930 - accuracy: 0.8205 - val_loss: 0.8822 - val_accuracy: 0.8248\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.8698 - accuracy: 0.8240 - val_loss: 0.8599 - val_accuracy: 0.8276\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.8486 - accuracy: 0.8266 - val_loss: 0.8396 - val_accuracy: 0.8301\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 0.8291 - accuracy: 0.8293 - val_loss: 0.8210 - val_accuracy: 0.8327\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.8112 - accuracy: 0.8315 - val_loss: 0.8037 - val_accuracy: 0.8337\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 0.7947 - accuracy: 0.8332 - val_loss: 0.7878 - val_accuracy: 0.8361\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 2s 11ms/step - loss: 0.7793 - accuracy: 0.8349 - val_loss: 0.7730 - val_accuracy: 0.8376\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.7650 - accuracy: 0.8369 - val_loss: 0.7593 - val_accuracy: 0.8401\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.7516 - accuracy: 0.8393 - val_loss: 0.7464 - val_accuracy: 0.8413\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.7392 - accuracy: 0.8409 - val_loss: 0.7343 - val_accuracy: 0.8440\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.7274 - accuracy: 0.8425 - val_loss: 0.7230 - val_accuracy: 0.8456\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.7164 - accuracy: 0.8440 - val_loss: 0.7124 - val_accuracy: 0.8476\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.7061 - accuracy: 0.8455 - val_loss: 0.7024 - val_accuracy: 0.8481\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.6963 - accuracy: 0.8471 - val_loss: 0.6929 - val_accuracy: 0.84901s -\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 2s 6ms/step - loss: 0.6870 - accuracy: 0.8483 - val_loss: 0.6840 - val_accuracy: 0.8510\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6783 - accuracy: 0.8491 - val_loss: 0.6755 - val_accuracy: 0.8526\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6699 - accuracy: 0.8503 - val_loss: 0.6674 - val_accuracy: 0.8534\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6620 - accuracy: 0.8514 - val_loss: 0.6598 - val_accuracy: 0.8541\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6545 - accuracy: 0.8527 - val_loss: 0.6525 - val_accuracy: 0.8544\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6473 - accuracy: 0.8536 - val_loss: 0.6455 - val_accuracy: 0.8544\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6405 - accuracy: 0.8547 - val_loss: 0.6389 - val_accuracy: 0.8558\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.6340 - accuracy: 0.8550 - val_loss: 0.6326 - val_accuracy: 0.8570\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.6277 - accuracy: 0.8559 - val_loss: 0.6265 - val_accuracy: 0.8580\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 0.6217 - accuracy: 0.8574 - val_loss: 0.6207 - val_accuracy: 0.8585\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.6160 - accuracy: 0.8584 - val_loss: 0.6152 - val_accuracy: 0.8587\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6105 - accuracy: 0.8593 - val_loss: 0.6098 - val_accuracy: 0.8609\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.6052 - accuracy: 0.8602 - val_loss: 0.6047 - val_accuracy: 0.8614\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.6001 - accuracy: 0.8612 - val_loss: 0.5998 - val_accuracy: 0.8626\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.5952 - accuracy: 0.8619 - val_loss: 0.5950 - val_accuracy: 0.8633\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 0.5904 - accuracy: 0.8627 - val_loss: 0.5905 - val_accuracy: 0.8641\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 3s 13ms/step - loss: 0.5859 - accuracy: 0.8632 - val_loss: 0.5861 - val_accuracy: 0.8648s - loss: 0.5888 - ac\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.5815 - accuracy: 0.8641 - val_loss: 0.5818 - val_accuracy: 0.8651\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.5772 - accuracy: 0.8648 - val_loss: 0.5777 - val_accuracy: 0.8651\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.5731 - accuracy: 0.8654 - val_loss: 0.5737 - val_accuracy: 0.8655\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.5692 - accuracy: 0.8663 - val_loss: 0.5699 - val_accuracy: 0.8663\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.5653 - accuracy: 0.8669 - val_loss: 0.5662 - val_accuracy: 0.86631s - l\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.5616 - accuracy: 0.8670 - val_loss: 0.5626 - val_accuracy: 0.8667\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.5580 - accuracy: 0.8678 - val_loss: 0.5591 - val_accuracy: 0.8668\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5545 - accuracy: 0.8683 - val_loss: 0.5557 - val_accuracy: 0.8672\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 1s 5ms/step - loss: 0.5511 - accuracy: 0.8691 - val_loss: 0.5524 - val_accuracy: 0.8675\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5478 - accuracy: 0.8690 - val_loss: 0.5493 - val_accuracy: 0.8679\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5446 - accuracy: 0.8693 - val_loss: 0.5462 - val_accuracy: 0.8684\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.8700 - val_loss: 0.5432 - val_accuracy: 0.8682\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5385 - accuracy: 0.8703 - val_loss: 0.5403 - val_accuracy: 0.8684\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5356 - accuracy: 0.8708 - val_loss: 0.5374 - val_accuracy: 0.8694\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.5327 - accuracy: 0.8711 - val_loss: 0.5347 - val_accuracy: 0.8697\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.5299 - accuracy: 0.8719 - val_loss: 0.5320 - val_accuracy: 0.8701\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.5272 - accuracy: 0.8720 - val_loss: 0.5294 - val_accuracy: 0.8702\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.5246 - accuracy: 0.8725 - val_loss: 0.5268 - val_accuracy: 0.8707\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5220 - accuracy: 0.8730 - val_loss: 0.5243 - val_accuracy: 0.8714\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5195 - accuracy: 0.8733 - val_loss: 0.5219 - val_accuracy: 0.8719\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.8736 - val_loss: 0.5196 - val_accuracy: 0.8721\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.5147 - accuracy: 0.8743 - val_loss: 0.5172 - val_accuracy: 0.8721\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.5123 - accuracy: 0.8742 - val_loss: 0.5150 - val_accuracy: 0.8724\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.8746 - val_loss: 0.5128 - val_accuracy: 0.8724\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.5078 - accuracy: 0.8748 - val_loss: 0.5107 - val_accuracy: 0.8731\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5057 - accuracy: 0.8751 - val_loss: 0.5086 - val_accuracy: 0.8733\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5035 - accuracy: 0.8758 - val_loss: 0.5065 - val_accuracy: 0.8740\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5015 - accuracy: 0.8756 - val_loss: 0.5045 - val_accuracy: 0.8747\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4994 - accuracy: 0.8761 - val_loss: 0.5026 - val_accuracy: 0.8750\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4974 - accuracy: 0.8766 - val_loss: 0.5006 - val_accuracy: 0.8752\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4955 - accuracy: 0.8768 - val_loss: 0.4988 - val_accuracy: 0.8759\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4936 - accuracy: 0.8771 - val_loss: 0.4969 - val_accuracy: 0.8764\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4917 - accuracy: 0.8776 - val_loss: 0.4951 - val_accuracy: 0.8765\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.4899 - accuracy: 0.8777 - val_loss: 0.4934 - val_accuracy: 0.8765\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4881 - accuracy: 0.8780 - val_loss: 0.4917 - val_accuracy: 0.8767\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.4863 - accuracy: 0.8783 - val_loss: 0.4900 - val_accuracy: 0.8767\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4846 - accuracy: 0.8786 - val_loss: 0.4883 - val_accuracy: 0.8774\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4829 - accuracy: 0.8787 - val_loss: 0.4867 - val_accuracy: 0.8779\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.4813 - accuracy: 0.8789 - val_loss: 0.4851 - val_accuracy: 0.8779\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4796 - accuracy: 0.8793 - val_loss: 0.4835 - val_accuracy: 0.8782\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.4780 - accuracy: 0.8794 - val_loss: 0.4820 - val_accuracy: 0.8791\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.8797 - val_loss: 0.4805 - val_accuracy: 0.8789\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.4749 - accuracy: 0.8801 - val_loss: 0.4790 - val_accuracy: 0.8793\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.4734 - accuracy: 0.8804 - val_loss: 0.4776 - val_accuracy: 0.8794\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4719 - accuracy: 0.8807 - val_loss: 0.4762 - val_accuracy: 0.8798\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.4705 - accuracy: 0.8810 - val_loss: 0.4748 - val_accuracy: 0.8799\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.4690 - accuracy: 0.8811 - val_loss: 0.4734 - val_accuracy: 0.8804\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.4676 - accuracy: 0.8813 - val_loss: 0.4721 - val_accuracy: 0.8804\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.4663 - accuracy: 0.8818 - val_loss: 0.4707 - val_accuracy: 0.8806\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.4649 - accuracy: 0.8821 - val_loss: 0.4694 - val_accuracy: 0.8806\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.4636 - accuracy: 0.8823 - val_loss: 0.4681 - val_accuracy: 0.8806\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.4622 - accuracy: 0.8823 - val_loss: 0.4669 - val_accuracy: 0.8806\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.4609 - accuracy: 0.8826 - val_loss: 0.4656 - val_accuracy: 0.8804\n"
     ]
    }
   ],
   "source": [
    "# model compile\n",
    "# 사용할 loss 함수를 지정, 사용한 optimizer(알고리즘)를 지정\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# loss\n",
    "# linear regression : linear\n",
    "# binary classification : binary_crossentropy\n",
    "# multinomial classification : categorical_crossentropy (onehot encoding 처리를 해야한다.)\n",
    "# multinomial classification : sparse_categorical_crossentropy (onehot 처리가 필요 없다.)\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 학습결과를 변수에 저장\n",
    "history = model.fit(norm_train_x_data,\n",
    "                    train_t_data,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e0cd092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 1s 4ms/step - loss: 0.4785 - accuracy: 0.8764\n",
      "[0.47846972942352295, 0.8764285445213318]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#         loss                accuracy\n",
    "# [0.47846972942352295, 0.8764285445213318]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 만든 모델을 저장해보자!\n",
    "# 학습한 후 모델이 메모리에 저장되어 있다. 프로그램 종료하면 다 날라간다!\n",
    "# 내일 다시 하려면 처음부터 다시 학습해야한다! => 시간이 오래 걸린다.\n",
    "\n",
    "# 모델 학습에 시간이 너무 오래 걸리는 경우\n",
    "# 중간에 미리 저장해놓으면 거기서부터 재 학습이 가능!\n",
    "\n",
    "# 다른 사람과 모델 공유가 가능!\n",
    "\n",
    "# 저장을 할 때 2가지 방법이 있다.\n",
    "# 모델을 저장할 때 모델 구조와 계산된 W,b를 같이 저장할 수 있다.\n",
    "# 장점 => 편하다!  단점 => 사이즈 크다!\n",
    "\n",
    "# 모델을 저장할 때 모델 구조는 저장하지 않고 W,b만 저장\n",
    "# 장점 => 크기가 작다. 단점 => 사용하려면 모델을 먼저 만들고 W,b를 로딩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02cfd9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "225/236 [===========================>..] - ETA: 0s - loss: 2.1500 - accuracy: 0.2305\n",
      "Epoch 00001: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 13ms/step - loss: 2.1451 - accuracy: 0.2356 - val_loss: 2.0373 - val_accuracy: 0.3432\n",
      "Epoch 2/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 1.9433 - accuracy: 0.4259\n",
      "Epoch 00002: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 1.9431 - accuracy: 0.4262 - val_loss: 1.8528 - val_accuracy: 0.4968\n",
      "Epoch 3/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 1.7753 - accuracy: 0.5489\n",
      "Epoch 00003: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 1.7739 - accuracy: 0.5500 - val_loss: 1.6978 - val_accuracy: 0.5964\n",
      "Epoch 4/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 1.6320 - accuracy: 0.6327\n",
      "Epoch 00004: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 1.6317 - accuracy: 0.6329 - val_loss: 1.5675 - val_accuracy: 0.6600\n",
      "Epoch 5/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 1.5131 - accuracy: 0.6793\n",
      "Epoch 00005: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 1.5122 - accuracy: 0.6798 - val_loss: 1.4578 - val_accuracy: 0.6937\n",
      "Epoch 6/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 1.4113 - accuracy: 0.7079\n",
      "Epoch 00006: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 1.4112 - accuracy: 0.7078 - val_loss: 1.3649 - val_accuracy: 0.7189\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 1.3254 - accuracy: 0.7301 ETA: 0s - loss: 1.3296 - accura\n",
      "Epoch 00007: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 13ms/step - loss: 1.3254 - accuracy: 0.7301 - val_loss: 1.2858 - val_accuracy: 0.7364\n",
      "Epoch 8/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 1.2531 - accuracy: 0.7466 ETA: 0s - loss: 1.2578 - accu\n",
      "Epoch 00008: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 11ms/step - loss: 1.2521 - accuracy: 0.7468 - val_loss: 1.2178 - val_accuracy: 0.7524\n",
      "Epoch 9/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 1.1891 - accuracy: 0.7617\n",
      "Epoch 00009: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 1.1889 - accuracy: 0.7617 - val_loss: 1.1591 - val_accuracy: 0.7646\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 1.1340 - accuracy: 0.7714\n",
      "Epoch 00010: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 11ms/step - loss: 1.1340 - accuracy: 0.7714 - val_loss: 1.1078 - val_accuracy: 0.7767\n",
      "Epoch 11/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 1.0860 - accuracy: 0.7810 ETA: 1s - l - ETA: 0s - loss: 1.0889 - accuracy: 0.\n",
      "Epoch 00011: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 1.0860 - accuracy: 0.7810 - val_loss: 1.0629 - val_accuracy: 0.7849\n",
      "Epoch 12/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 1.0438 - accuracy: 0.7880 ETA: 1s - loss: 1.0525 - accuracy: 0. - ETA: 1s - loss: 1.0521 - accura - ETA:  - ETA: 0s - loss: 1.0445 - accu\n",
      "Epoch 00012: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 4s 16ms/step - loss: 1.0436 - accuracy: 0.7881 - val_loss: 1.0231 - val_accuracy: 0.7935\n",
      "Epoch 13/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 1.0059 - accuracy: 0.7942 ETA - ETA\n",
      "Epoch 00013: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 1.0060 - accuracy: 0.7941 - val_loss: 0.9877 - val_accuracy: 0.7997\n",
      "Epoch 14/100\n",
      "220/236 [==========================>...] - ETA: 0s - loss: 0.9755 - accuracy: 0.7982 ETA: 0s - loss: 0.9798 - accu - ETA: 0s - loss: 0.9785 - \n",
      "Epoch 00014: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.9725 - accuracy: 0.7991 - val_loss: 0.9560 - val_accuracy: 0.8026\n",
      "Epoch 15/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.9412 - accuracy: 0.8047\n",
      "Epoch 00015: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.9423 - accuracy: 0.8034 - val_loss: 0.9274 - val_accuracy: 0.8099\n",
      "Epoch 16/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.9156 - accuracy: 0.8076\n",
      "Epoch 00016: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.9150 - accuracy: 0.8075 - val_loss: 0.9016 - val_accuracy: 0.8143\n",
      "Epoch 17/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.8903 - accuracy: 0.8112\n",
      "Epoch 00017: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.8903 - accuracy: 0.8113 - val_loss: 0.8780 - val_accuracy: 0.8177\n",
      "Epoch 18/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 0.8680 - accuracy: 0.8143\n",
      "Epoch 00018: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 11ms/step - loss: 0.8677 - accuracy: 0.8147 - val_loss: 0.8566 - val_accuracy: 0.8197\n",
      "Epoch 19/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 0.8471 - accuracy: 0.8182\n",
      "Epoch 00019: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 0.8470 - accuracy: 0.8182 - val_loss: 0.8369 - val_accuracy: 0.8233\n",
      "Epoch 20/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.8282 - accuracy: 0.8216\n",
      "Epoch 00020: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.8280 - accuracy: 0.8213 - val_loss: 0.8188 - val_accuracy: 0.8260\n",
      "Epoch 21/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.8105 - accuracy: 0.8243 ETA: 0s - loss: 0.8\n",
      "Epoch 00021: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.8105 - accuracy: 0.8242 - val_loss: 0.8019 - val_accuracy: 0.8291\n",
      "Epoch 22/100\n",
      "227/236 [===========================>..] - ETA: 0s - loss: 0.7950 - accuracy: 0.8264 ETA: 0s - loss: 0.7964 - accuracy\n",
      "Epoch 00022: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.7942 - accuracy: 0.8266 - val_loss: 0.7864 - val_accuracy: 0.8306\n",
      "Epoch 23/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.7789 - accuracy: 0.8288 ETA: 1s - loss: 0.7742  - ETA: 1s\n",
      "Epoch 00023: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.7791 - accuracy: 0.8287 - val_loss: 0.7719 - val_accuracy: 0.8332\n",
      "Epoch 24/100\n",
      "225/236 [===========================>..] - ETA: 0s - loss: 0.7655 - accuracy: 0.8309\n",
      "Epoch 00024: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.7650 - accuracy: 0.8315 - val_loss: 0.7584 - val_accuracy: 0.8347\n",
      "Epoch 25/100\n",
      "226/236 [===========================>..] - ETA: 0s - loss: 0.7517 - accuracy: 0.8331\n",
      "Epoch 00025: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.7519 - accuracy: 0.8334 - val_loss: 0.7458 - val_accuracy: 0.8355\n",
      "Epoch 26/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.7384 - accuracy: 0.8361\n",
      "Epoch 00026: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.7396 - accuracy: 0.8354 - val_loss: 0.7339 - val_accuracy: 0.8376\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.7280 - accuracy: 0.8372\n",
      "Epoch 00027: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.7280 - accuracy: 0.8372 - val_loss: 0.7229 - val_accuracy: 0.8393\n",
      "Epoch 28/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.7171 - accuracy: 0.8389\n",
      "Epoch 00028: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.7171 - accuracy: 0.8389 - val_loss: 0.7124 - val_accuracy: 0.8408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 0.7068 - accuracy: 0.8408\n",
      "Epoch 00029: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.7069 - accuracy: 0.8406 - val_loss: 0.7025 - val_accuracy: 0.8422\n",
      "Epoch 30/100\n",
      "222/236 [===========================>..] - ETA: 0s - loss: 0.6980 - accuracy: 0.8423\n",
      "Epoch 00030: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6972 - accuracy: 0.8423 - val_loss: 0.6932 - val_accuracy: 0.8439\n",
      "Epoch 31/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.6891 - accuracy: 0.8434\n",
      "Epoch 00031: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.8443 - val_loss: 0.6844 - val_accuracy: 0.8446\n",
      "Epoch 32/100\n",
      "214/236 [==========================>...] - ETA: 0s - loss: 0.6790 - accuracy: 0.8460\n",
      "Epoch 00032: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.6793 - accuracy: 0.8459 - val_loss: 0.6761 - val_accuracy: 0.8456\n",
      "Epoch 33/100\n",
      "223/236 [===========================>..] - ETA: 0s - loss: 0.6727 - accuracy: 0.8470\n",
      "Epoch 00033: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.8474 - val_loss: 0.6681 - val_accuracy: 0.8469\n",
      "Epoch 34/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.6647 - accuracy: 0.8479\n",
      "Epoch 00034: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6632 - accuracy: 0.8485 - val_loss: 0.6606 - val_accuracy: 0.8480\n",
      "Epoch 35/100\n",
      "213/236 [==========================>...] - ETA: 0s - loss: 0.6556 - accuracy: 0.8497\n",
      "Epoch 00035: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.8495 - val_loss: 0.6534 - val_accuracy: 0.8493\n",
      "Epoch 36/100\n",
      "226/236 [===========================>..] - ETA: 0s - loss: 0.6488 - accuracy: 0.8505\n",
      "Epoch 00036: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.8506 - val_loss: 0.6465 - val_accuracy: 0.8503\n",
      "Epoch 37/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.6420 - accuracy: 0.8514\n",
      "Epoch 00037: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.8514 - val_loss: 0.6400 - val_accuracy: 0.8512\n",
      "Epoch 38/100\n",
      "211/236 [=========================>....] - ETA: 0s - loss: 0.6356 - accuracy: 0.8526\n",
      "Epoch 00038: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.8524 - val_loss: 0.6337 - val_accuracy: 0.8529\n",
      "Epoch 39/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.6289 - accuracy: 0.8533\n",
      "Epoch 00039: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.6290 - accuracy: 0.8531 - val_loss: 0.6277 - val_accuracy: 0.8551\n",
      "Epoch 40/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 0.6232 - accuracy: 0.8547\n",
      "Epoch 00040: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.6231 - accuracy: 0.8547 - val_loss: 0.6219 - val_accuracy: 0.8565\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.6173 - accuracy: 0.8552 ETA: 1s - l - ETA: 0s - loss: 0.6169 - accura\n",
      "Epoch 00041: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 3s 12ms/step - loss: 0.6173 - accuracy: 0.8552 - val_loss: 0.6164 - val_accuracy: 0.8571\n",
      "Epoch 42/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.6117 - accuracy: 0.8566\n",
      "Epoch 00042: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 6ms/step - loss: 0.6118 - accuracy: 0.8566 - val_loss: 0.6111 - val_accuracy: 0.8575\n",
      "Epoch 43/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.6062 - accuracy: 0.8575\n",
      "Epoch 00043: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.6065 - accuracy: 0.8573 - val_loss: 0.6060 - val_accuracy: 0.8592\n",
      "Epoch 44/100\n",
      "208/236 [=========================>....] - ETA: 0s - loss: 0.6000 - accuracy: 0.8593 ETA: 0s - loss: 0.609\n",
      "Epoch 00044: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.6015 - accuracy: 0.8580 - val_loss: 0.6012 - val_accuracy: 0.8597\n",
      "Epoch 45/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.5968 - accuracy: 0.8588\n",
      "Epoch 00045: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.5966 - accuracy: 0.8588 - val_loss: 0.5964 - val_accuracy: 0.8602\n",
      "Epoch 46/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.5917 - accuracy: 0.8600\n",
      "Epoch 00046: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.5919 - accuracy: 0.8599 - val_loss: 0.5919 - val_accuracy: 0.8607\n",
      "Epoch 47/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 0.5882 - accuracy: 0.8600\n",
      "Epoch 00047: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.5873 - accuracy: 0.8604 - val_loss: 0.5875 - val_accuracy: 0.8617\n",
      "Epoch 48/100\n",
      "205/236 [=========================>....] - ETA: 0s - loss: 0.5834 - accuracy: 0.8616\n",
      "Epoch 00048: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.5829 - accuracy: 0.8614 - val_loss: 0.5833 - val_accuracy: 0.8628\n",
      "Epoch 49/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.5787 - accuracy: 0.8619\n",
      "Epoch 00049: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5787 - accuracy: 0.8622 - val_loss: 0.5792 - val_accuracy: 0.8628\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.5746 - accuracy: 0.8626 E\n",
      "Epoch 00050: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.5746 - accuracy: 0.8626 - val_loss: 0.5752 - val_accuracy: 0.8631\n",
      "Epoch 51/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.8646\n",
      "Epoch 00051: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5706 - accuracy: 0.8640 - val_loss: 0.5714 - val_accuracy: 0.8643\n",
      "Epoch 52/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 0.5666 - accuracy: 0.8645\n",
      "Epoch 00052: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5668 - accuracy: 0.8644 - val_loss: 0.5677 - val_accuracy: 0.8645\n",
      "Epoch 53/100\n",
      "228/236 [===========================>..] - ETA: 0s - loss: 0.5622 - accuracy: 0.8655\n",
      "Epoch 00053: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.5631 - accuracy: 0.8649 - val_loss: 0.5642 - val_accuracy: 0.8658\n",
      "Epoch 54/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.5587 - accuracy: 0.8655\n",
      "Epoch 00054: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.5595 - accuracy: 0.8651 - val_loss: 0.5607 - val_accuracy: 0.8665\n",
      "Epoch 55/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.5574 - accuracy: 0.8657\n",
      "Epoch 00055: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5560 - accuracy: 0.8661 - val_loss: 0.5573 - val_accuracy: 0.8672\n",
      "Epoch 56/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.5526 - accuracy: 0.8669\n",
      "Epoch 00056: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.5526 - accuracy: 0.8669 - val_loss: 0.5541 - val_accuracy: 0.8679\n",
      "Epoch 57/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.5493 - accuracy: 0.8673\n",
      "Epoch 00057: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5493 - accuracy: 0.8672 - val_loss: 0.5509 - val_accuracy: 0.8684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.5468 - accuracy: 0.8677\n",
      "Epoch 00058: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5461 - accuracy: 0.8676 - val_loss: 0.5478 - val_accuracy: 0.8684\n",
      "Epoch 59/100\n",
      "213/236 [==========================>...] - ETA: 0s - loss: 0.5449 - accuracy: 0.8671\n",
      "Epoch 00059: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5430 - accuracy: 0.8680 - val_loss: 0.5449 - val_accuracy: 0.8680\n",
      "Epoch 60/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.5400 - accuracy: 0.8686\n",
      "Epoch 00060: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5400 - accuracy: 0.8686 - val_loss: 0.5419 - val_accuracy: 0.8680\n",
      "Epoch 61/100\n",
      "222/236 [===========================>..] - ETA: 0s - loss: 0.5364 - accuracy: 0.8698\n",
      "Epoch 00061: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5370 - accuracy: 0.8690 - val_loss: 0.5391 - val_accuracy: 0.8685\n",
      "Epoch 62/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.5343 - accuracy: 0.8695\n",
      "Epoch 00062: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5342 - accuracy: 0.8695 - val_loss: 0.5364 - val_accuracy: 0.8696\n",
      "Epoch 63/100\n",
      "194/236 [=======================>......] - ETA: 0s - loss: 0.5326 - accuracy: 0.8691\n",
      "Epoch 00063: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5314 - accuracy: 0.8698 - val_loss: 0.5337 - val_accuracy: 0.8696\n",
      "Epoch 64/100\n",
      "227/236 [===========================>..] - ETA: 0s - loss: 0.5290 - accuracy: 0.8704\n",
      "Epoch 00064: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5287 - accuracy: 0.8702 - val_loss: 0.5311 - val_accuracy: 0.8711\n",
      "Epoch 65/100\n",
      "219/236 [==========================>...] - ETA: 0s - loss: 0.5246 - accuracy: 0.8710\n",
      "Epoch 00065: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.8710 - val_loss: 0.5285 - val_accuracy: 0.8709\n",
      "Epoch 66/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.5254 - accuracy: 0.8706\n",
      "Epoch 00066: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.8713 - val_loss: 0.5261 - val_accuracy: 0.8716\n",
      "Epoch 67/100\n",
      "221/236 [===========================>..] - ETA: 0s - loss: 0.5211 - accuracy: 0.8716\n",
      "Epoch 00067: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5210 - accuracy: 0.8719 - val_loss: 0.5237 - val_accuracy: 0.8719\n",
      "Epoch 68/100\n",
      "225/236 [===========================>..] - ETA: 0s - loss: 0.5184 - accuracy: 0.8722\n",
      "Epoch 00068: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.5185 - accuracy: 0.8724 - val_loss: 0.5213 - val_accuracy: 0.8723\n",
      "Epoch 69/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.5162 - accuracy: 0.8725\n",
      "Epoch 00069: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.5161 - accuracy: 0.8726 - val_loss: 0.5190 - val_accuracy: 0.8724\n",
      "Epoch 70/100\n",
      "222/236 [===========================>..] - ETA: 0s - loss: 0.5131 - accuracy: 0.8726\n",
      "Epoch 00070: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.5138 - accuracy: 0.8730 - val_loss: 0.5167 - val_accuracy: 0.8731\n",
      "Epoch 71/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.5118 - accuracy: 0.8733\n",
      "Epoch 00071: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.5115 - accuracy: 0.8734 - val_loss: 0.5145 - val_accuracy: 0.8733\n",
      "Epoch 72/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 0.5085 - accuracy: 0.8749\n",
      "Epoch 00072: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.5093 - accuracy: 0.8743 - val_loss: 0.5124 - val_accuracy: 0.8736\n",
      "Epoch 73/100\n",
      "231/236 [============================>.] - ETA: 0s - loss: 0.5072 - accuracy: 0.8737\n",
      "Epoch 00073: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.5071 - accuracy: 0.8740 - val_loss: 0.5103 - val_accuracy: 0.8741\n",
      "Epoch 74/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.5048 - accuracy: 0.8748 ETA: \n",
      "Epoch 00074: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 8ms/step - loss: 0.5050 - accuracy: 0.8744 - val_loss: 0.5083 - val_accuracy: 0.8740\n",
      "Epoch 75/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.5035 - accuracy: 0.8750\n",
      "Epoch 00075: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.5029 - accuracy: 0.8750 - val_loss: 0.5063 - val_accuracy: 0.8747\n",
      "Epoch 76/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.8750 ETA: 0s - loss: 0.5025 - accu\n",
      "Epoch 00076: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.5008 - accuracy: 0.8749 - val_loss: 0.5043 - val_accuracy: 0.8750\n",
      "Epoch 77/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.4976 - accuracy: 0.8759\n",
      "Epoch 00077: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 6ms/step - loss: 0.4988 - accuracy: 0.8755 - val_loss: 0.5024 - val_accuracy: 0.8752\n",
      "Epoch 78/100\n",
      "229/236 [============================>.] - ETA: 0s - loss: 0.4959 - accuracy: 0.8758\n",
      "Epoch 00078: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.4969 - accuracy: 0.8756 - val_loss: 0.5005 - val_accuracy: 0.8759\n",
      "Epoch 79/100\n",
      "230/236 [============================>.] - ETA: 0s - loss: 0.4953 - accuracy: 0.8758\n",
      "Epoch 00079: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.4950 - accuracy: 0.8759 - val_loss: 0.4987 - val_accuracy: 0.8759\n",
      "Epoch 80/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.4921 - accuracy: 0.8765\n",
      "Epoch 00080: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.4931 - accuracy: 0.8761 - val_loss: 0.4969 - val_accuracy: 0.8762\n",
      "Epoch 81/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 0.4911 - accuracy: 0.8766 E\n",
      "Epoch 00081: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.4913 - accuracy: 0.8766 - val_loss: 0.4952 - val_accuracy: 0.8762\n",
      "Epoch 82/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.8768 ETA: 0s - loss: 0.4837 - accura\n",
      "Epoch 00082: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.4895 - accuracy: 0.8767 - val_loss: 0.4934 - val_accuracy: 0.8770\n",
      "Epoch 83/100\n",
      "218/236 [==========================>...] - ETA: 0s - loss: 0.4872 - accuracy: 0.8778\n",
      "Epoch 00083: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.4877 - accuracy: 0.8773 - val_loss: 0.4917 - val_accuracy: 0.8769\n",
      "Epoch 84/100\n",
      "234/236 [============================>.] - ETA: 0s - loss: 0.4860 - accuracy: 0.8777 ETA: 0s - loss: 0.4867 \n",
      "Epoch 00084: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.4860 - accuracy: 0.8778 - val_loss: 0.4901 - val_accuracy: 0.8774\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.8781\n",
      "Epoch 00085: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.4843 - accuracy: 0.8781 - val_loss: 0.4885 - val_accuracy: 0.8777\n",
      "Epoch 86/100\n",
      "224/236 [===========================>..] - ETA: 0s - loss: 0.4835 - accuracy: 0.8788\n",
      "Epoch 00086: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.4826 - accuracy: 0.8786 - val_loss: 0.4869 - val_accuracy: 0.8779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.4810 - accuracy: 0.8788\n",
      "Epoch 00087: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4810 - accuracy: 0.8788 - val_loss: 0.4853 - val_accuracy: 0.8782\n",
      "Epoch 88/100\n",
      "232/236 [============================>.] - ETA: 0s - loss: 0.4797 - accuracy: 0.8790\n",
      "Epoch 00088: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.4794 - accuracy: 0.8793 - val_loss: 0.4838 - val_accuracy: 0.8789\n",
      "Epoch 89/100\n",
      "219/236 [==========================>...] - ETA: 0s - loss: 0.4779 - accuracy: 0.8796\n",
      "Epoch 00089: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4778 - accuracy: 0.8795 - val_loss: 0.4823 - val_accuracy: 0.8787\n",
      "Epoch 90/100\n",
      "210/236 [=========================>....] - ETA: 0s - loss: 0.4786 - accuracy: 0.8788\n",
      "Epoch 00090: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.4763 - accuracy: 0.8795 - val_loss: 0.4808 - val_accuracy: 0.8787\n",
      "Epoch 91/100\n",
      "212/236 [=========================>....] - ETA: 0s - loss: 0.4767 - accuracy: 0.8784\n",
      "Epoch 00091: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4748 - accuracy: 0.8797 - val_loss: 0.4793 - val_accuracy: 0.8794\n",
      "Epoch 92/100\n",
      "235/236 [============================>.] - ETA: 0s - loss: 0.4733 - accuracy: 0.8799\n",
      "Epoch 00092: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.4733 - accuracy: 0.8799 - val_loss: 0.4779 - val_accuracy: 0.8791\n",
      "Epoch 93/100\n",
      "222/236 [===========================>..] - ETA: 0s - loss: 0.4739 - accuracy: 0.8798\n",
      "Epoch 00093: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.4718 - accuracy: 0.8801 - val_loss: 0.4765 - val_accuracy: 0.8796\n",
      "Epoch 94/100\n",
      "213/236 [==========================>...] - ETA: 0s - loss: 0.4720 - accuracy: 0.8802\n",
      "Epoch 00094: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 4ms/step - loss: 0.4704 - accuracy: 0.8802 - val_loss: 0.4751 - val_accuracy: 0.8799\n",
      "Epoch 95/100\n",
      "180/236 [=====================>........] - ETA: 0s - loss: 0.4676 - accuracy: 0.8807\n",
      "Epoch 00095: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4690 - accuracy: 0.8810 - val_loss: 0.4738 - val_accuracy: 0.8798\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.8810\n",
      "Epoch 00096: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.8810 - val_loss: 0.4725 - val_accuracy: 0.8799\n",
      "Epoch 97/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.4663 - accuracy: 0.8810\n",
      "Epoch 00097: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4662 - accuracy: 0.8811 - val_loss: 0.4712 - val_accuracy: 0.8803\n",
      "Epoch 98/100\n",
      "189/236 [=======================>......] - ETA: 0s - loss: 0.4668 - accuracy: 0.8810\n",
      "Epoch 00098: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4649 - accuracy: 0.8813 - val_loss: 0.4699 - val_accuracy: 0.8804\n",
      "Epoch 99/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.4637 - accuracy: 0.8815\n",
      "Epoch 00099: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4635 - accuracy: 0.8814 - val_loss: 0.4686 - val_accuracy: 0.8806\n",
      "Epoch 100/100\n",
      "233/236 [============================>.] - ETA: 0s - loss: 0.4625 - accuracy: 0.8815\n",
      "Epoch 00100: saving model to ../training_ckpt\\cp.ckpt\n",
      "236/236 [==============================] - 1s 3ms/step - loss: 0.4622 - accuracy: 0.8817 - val_loss: 0.4674 - val_accuracy: 0.8806\n",
      "394/394 [==============================] - 1s 2ms/step - loss: 0.4784 - accuracy: 0.8759\n",
      "[0.4783697724342346, 0.8758730292320251]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential   # keras model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(input Layer)\n",
    "                                                     # Dense(Output Layer)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd. read_csv('../data/mnist/train.csv')\n",
    "\n",
    "# Data Split\n",
    "# 기존에는 test_x_data, test_t_data 이 두 데이터를 validation 용도로 사용했다.\n",
    "# 이제는 test_x_data, test_t_data 이 두 데이터를 test 용도로 사용할 것이다.\n",
    "# 최종 모델 성능 평가를 위해서 딱 1번만 사용할 것이다.\n",
    "# 그러면 validation은 어떻게 하나요?\n",
    "# keras는 학습할 때 train data를 일정부분 나누어서 자체 validation이 가능\n",
    "# keras 기능을 이용해서 validation 처리\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n",
    "\n",
    "# 우리는 loss 지정할 때 sparse_categorical_crossentropy로 loss 함수를\n",
    "# 지정할 예정이기 때문에 label에 대한 one-hot encoding 처리가 필요 없다!\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()   # model 박스 하나 만듬\n",
    "\n",
    "# print(norm_train_x_data.shape)   # (29400, 784)\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "\n",
    "# model compile\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model을 저장하려고 한다. model 구조 빼고 checkpoint기능을 이용해서\n",
    "# weight, b만 저장\n",
    "# 어디에 저장할지를 알려줘야 한다.\n",
    "checkpoint_path = '../training_ckpt/cp.ckpt'\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)  # 실제 경로로 만든다!\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                             save_weights_only=True,   # Weight만 저장하겠다.\n",
    "                             verbose=1)\n",
    "\n",
    "\n",
    "# 학습결과를 변수에 저장\n",
    "history = model.fit(norm_train_x_data,\n",
    "                    train_t_data,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    verbose=1,             # 0으로 하면 결과가 안나온다.\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "# 우리 모델에 대한 최종평가 진행\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#         loss                accuracy\n",
    "# [0.47846972942352295, 0.8764285445213318]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f19aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "394/394 [==============================] - 3s 7ms/step - loss: 2.4126 - accuracy: 0.0720\n",
      "[2.4125711917877197, 0.07198412716388702]\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "# 아하... 이렇게 저장할 수 있다!\n",
    "# 불러서 다시 사용하려면 어떻게 해야 할까?\n",
    "\n",
    "# 확인하기 위해...\n",
    "# 일단 학습하지 않은 상태로 evaluation을 진행하면 당연히 평가 결과가\n",
    "# 좋지 않을 것이다. 이것을 먼저 확인하고\n",
    "# 그 다음에 checkpoint 파일을 로드해서 model을 재설정하고 평가를 진행한다.\n",
    "# 좋게 나올 것이다!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential   # keras model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Flatten(input Layer)\n",
    "                                                     # Dense(Output Layer)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd. read_csv('../data/mnist/train.csv')\n",
    "\n",
    "# Data Split\n",
    "# 기존에는 test_x_data, test_t_data 이 두 데이터를 validation 용도로 사용했다.\n",
    "# 이제는 test_x_data, test_t_data 이 두 데이터를 test 용도로 사용할 것이다.\n",
    "# 최종 모델 성능 평가를 위해서 딱 1번만 사용할 것이다.\n",
    "# 그러면 validation은 어떻게 하나요?\n",
    "# keras는 학습할 때 train data를 일정부분 나누어서 자체 validation이 가능\n",
    "# keras 기능을 이용해서 validation 처리\n",
    "\n",
    "train_x_data, test_x_data, train_t_data, test_t_data = \\\n",
    "train_test_split(df.drop('label', axis=1, inplace=False),\n",
    "                 df['label'],\n",
    "                 test_size=0.3,\n",
    "                 random_state=1,\n",
    "                 stratify=df['label'])\n",
    "\n",
    "# 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x_data)\n",
    "\n",
    "norm_train_x_data = scaler.transform(train_x_data)\n",
    "norm_test_x_data = scaler.transform(test_x_data)\n",
    "\n",
    "# 우리는 loss 지정할 때 sparse_categorical_crossentropy로 loss 함수를\n",
    "# 지정할 예정이기 때문에 label에 대한 one-hot encoding 처리가 필요 없다!\n",
    "\n",
    "# model 생성\n",
    "model = Sequential()   # model 박스 하나 만듬\n",
    "\n",
    "# print(norm_train_x_data.shape)   # (29400, 784)\n",
    "\n",
    "# layer 추가\n",
    "# input layer\n",
    "model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=10,\n",
    "                activation='softmax'))\n",
    "\n",
    "# model compile\n",
    "model.compile(optimizer=SGD(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 원래는 학습을 진행해야한다! 그런데 학습을 진행하지 않을 것이다!\n",
    "\n",
    "# 학습을 진행하지 않고 최종평가 진행\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#         loss                accuracy\n",
    "# [2.414344072341919, 0.10619047284126282]\n",
    "# 당연히 학습이 안된 모델이기 때문에 이렇게 나오는 게 정상이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "949651ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394/394 [==============================] - 3s 8ms/step - loss: 0.4784 - accuracy: 0.8759\n",
      "[0.4783697724342346, 0.8758730292320251]\n"
     ]
    }
   ],
   "source": [
    "# 이번에는 checkpoint 파일에 있는 weight를 load한 후\n",
    "# evaluation 시켜보자!\n",
    "\n",
    "checkpoint_path = '../training_ckpt/cp.ckpt'\n",
    "model.load_weights(checkpoint_path)\n",
    "print(model.evaluate(norm_test_x_data, test_t_data))\n",
    "#         loss                accuracy\n",
    "# [0.4783697724342346, 0.8758730292320251]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF2] *",
   "language": "python",
   "name": "conda-env-machine_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
